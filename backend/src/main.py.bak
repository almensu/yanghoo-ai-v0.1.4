import logging
import os # Keep os for makedirs etc.
from pathlib import Path # Import Path
import shutil
import json
from uuid import UUID
from typing import Dict, List, Optional, Literal, Any
from datetime import datetime, timezone # Added import
import aiofiles
import yt_dlp
import ffmpeg
import torch
import subprocess
import sys
import asyncio
import mimetypes # 添加 mimetypes 导入
import uuid
import shlex
from fastapi import BackgroundTasks, Body # Import BackgroundTasks and Body
from pydantic import BaseModel, Field # Import BaseModel and Field
import re
import copy
import httpx # Add httpx for async HTTP requests
import time
import platform
from concurrent.futures import ThreadPoolExecutor
from starlette.concurrency import run_in_threadpool

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect, Query, Path as PathParam
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
# from pathlib import Path # Remove Path import
from pydantic import ValidationError, BaseModel
from pydantic.json import pydantic_encoder
from fastapi.responses import FileResponse # Import FileResponse

# --- Schemas Import --- 
from .schemas import (
    IngestRequest, IngestResponse, TaskMetadata, 
    FetchInfoJsonResponse, 
    DownloadMediaRequest, DownloadMediaResponse,
    ExtractAudioResponse,
    TranscribeRequest, TranscribeResponse, Platform,
    MergeResponse,
    DownloadAudioResponse
)
# --- Tasks Import --- 
from .tasks.ingest import create_ingest_task
from .tasks.fetch_info_json import run_fetch_info_json
from .tasks.download_media import run_download_media
from .tasks.extract_audio import run_extract_audio
from .tasks.transcribe_youtube_vtt import run_transcribe_youtube_vtt
from .tasks.merge_vtt import main as run_merge_vtt
from .tasks.merge_whisperx import run_merge_whisperx
from .tasks.download_youtueb_vtt import download_youtube_vtt
from .tasks.download_audio import download_audio_sync, AUDIO_DOWNLOAD_PLATFORMS
# --- Data Management Import --- 
from .data_management import delete_video_files_sync, delete_audio_file_sync
# --- Restore Archived Task Import ---
from .tasks.Restore_Archived import restore_archived_metadata
# --- Audio to Media Task Import ---
from .tasks.audio_to_media import create_video_from_audio_image
# --- 工具函数导入 ---
from .utils import extract_youtube_video_id # 导入提取函数
# ------------------------
from fastapi.concurrency import run_in_threadpool

# Use pathlib for paths
SRC_DIR = Path(__file__).parent.resolve()
BACKEND_DIR = SRC_DIR.parent
DATA_DIR = BACKEND_DIR / "data"

app = FastAPI()

# --- Add CORS Middleware --- 
# Allow origins (e.g., your frontend development server)
origins = [
    "http://localhost:3000", # React default dev port
    "http://127.0.0.1:3000", # Also allow 127.0.0.1 for the frontend
    "http://localhost:8080", # Vue default dev port (example)
    "http://localhost:4200", # Angular default dev port (example)
    # Add any other origins you might use (e.g., deployed frontend URL)
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"], # Allow all methods (GET, POST, DELETE, etc.)
    allow_headers=["*"], # Allow all headers
)
# --- End CORS Middleware ---

# Ensure the directory exists using pathlib
DATA_DIR.mkdir(parents=True, exist_ok=True)

# Mount the calculated 'data' directory path (StaticFiles needs a string)
app.mount("/files", StaticFiles(directory=str(DATA_DIR)), name="static_files")

# Define base directory and metadata file path using pathlib
BASE_DIR = DATA_DIR # BASE_DIR is now a Path object
METADATA_FILE = BASE_DIR / "metadata.json"
# --- NEW: Define Archive File Path --- 
METADATA_ARCHIVED_FILE = BASE_DIR / "metadata_archived.json"

# --- WebSocket Connection Manager ---
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        logger.info(f"WebSocket connection established: {websocket.client}")

    def disconnect(self, websocket: WebSocket):
        # Add check before removing to prevent race condition errors
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
            logger.info(f"WebSocket connection closed: {websocket.client}")
        else:
            logger.warning(f"Attempted to disconnect websocket already removed: {websocket.client}")

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def send_json(self, data: Dict[str, Any], websocket: WebSocket):
        await websocket.send_json(data)
        
    async def broadcast(self, data: Dict[str, Any]):
        disconnected_clients = []
        message_str = json.dumps(data, default=pydantic_encoder) # Use pydantic_encoder if broadcasting TaskMetadata
        logger.info(f"Broadcasting message to {len(self.active_connections)} clients: {message_str[:200]}...")
        for connection in self.active_connections:
            try:
                await connection.send_json(data)
            except WebSocketDisconnect:
                logger.warning(f"Client disconnected during broadcast: {connection.client}")
                disconnected_clients.append(connection)
            except Exception as e:
                logger.error(f"Error sending message to client {connection.client}: {e}", exc_info=False) # Log less verbosely
                disconnected_clients.append(connection) # Also remove clients causing errors
                
        # Clean up disconnected clients after broadcasting
        for client in disconnected_clients:
            if client in self.active_connections: # Check if not already removed by another process/exception
                 self.disconnect(client)

manager = ConnectionManager()
# --- End WebSocket Connection Manager ---

# --- 新增：填充 embed_url 的辅助函数 ---
def _populate_embed_url(task_meta: TaskMetadata):
    """Helper to populate embed_url if platform is YouTube."""
    if task_meta.platform == Platform.YOUTUBE and task_meta.url:
        video_id = extract_youtube_video_id(task_meta.url)
        if video_id:
            # Pydantic v2 会自动验证 HttpUrl，如果格式无效会抛错，需要处理
            try:
                task_meta.embed_url = f"https://www.youtube.com/embed/{video_id}"
            except Exception as e: # 捕获可能的验证错误
                logger.warning(f"无法为任务 {task_meta.uuid} 构建或验证有效的 embed_url: {e}")
                task_meta.embed_url = None # 确保无效时不设置
    return task_meta
# --- 结束辅助函数 ---

async def load_metadata() -> Dict[str, TaskMetadata]:
    if not METADATA_FILE.exists(): # Use pathlib
        return {}
    valid_metadata = {} 
    try:
        # Use METADATA_FILE (Path object) directly with aiofiles
        async with aiofiles.open(METADATA_FILE, mode='r') as f:
            content = await f.read()
            if not content.strip():
                 logger.warning(f"Metadata file is empty: {METADATA_FILE}")
                 return {}
            data = json.loads(content)
            for uuid_str, meta_dict in data.items():
                try:
                    task_meta = TaskMetadata(**meta_dict)
                    valid_metadata[uuid_str] = task_meta
                except ValidationError as e:
                    logger.warning(f"Skipping task {uuid_str} due to validation error: {e}")
            return valid_metadata
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"Error reading or parsing metadata file {METADATA_FILE}: {e}")
        return {}
    except Exception as e:
        logger.error(f"Unexpected error loading metadata: {e}", exc_info=True)
        return {}
    # Need to return valid_metadata here if successful
    return valid_metadata

async def save_metadata(metadata: Dict[str, TaskMetadata]):
    logger.info(f"Attempting to save metadata. Data to save: {metadata}") # Log the data
    try:
        # Use METADATA_FILE (Path object) directly with aiofiles
        async with aiofiles.open(METADATA_FILE, mode='w', encoding='utf-8') as f:
            content_to_write = json.dumps(metadata, indent=4, default=pydantic_encoder, ensure_ascii=False)
            logger.debug(f"Content prepared for writing to {METADATA_FILE}:\n{content_to_write}") # Log exact content
            await f.write(content_to_write)
        logger.info(f"Successfully saved metadata to {METADATA_FILE}") # Log success
    except IOError as e:
        logger.error(f"Error saving metadata to {METADATA_FILE}: {e}")
    except Exception as e:
        logger.error(f"Unexpected error during metadata save to {METADATA_FILE}: {e}", exc_info=True)

# --- NEW: Load/Save for Archived Metadata --- 
# Simple dictionary structure for archived data
async def load_archived_metadata() -> Dict[str, Dict]: 
    if not METADATA_ARCHIVED_FILE.exists():
        return {}
    try:
        async with aiofiles.open(METADATA_ARCHIVED_FILE, mode='r') as f:
            content = await f.read()
            if not content.strip():
                 logger.warning(f"Archived metadata file is empty: {METADATA_ARCHIVED_FILE}")
                 return {}
            # Basic JSON load, no Pydantic validation needed here
            data = json.loads(content)
            return data
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"Error reading or parsing archived metadata file {METADATA_ARCHIVED_FILE}: {e}")
        return {}
    except Exception as e:
        logger.error(f"Unexpected error loading archived metadata: {e}", exc_info=True)
        return {}

async def save_archived_metadata(metadata: Dict[str, Dict]):
    logger.info(f"Attempting to save archived metadata.")
    try:
        async with aiofiles.open(METADATA_ARCHIVED_FILE, mode='w') as f:
            # Use default JSON encoder, archived data is simple dict
            content_to_write = json.dumps(metadata, indent=4, ensure_ascii=False) 
            await f.write(content_to_write)
        logger.info(f"Successfully saved archived metadata to {METADATA_ARCHIVED_FILE}")
    except IOError as e:
        logger.error(f"Error saving archived metadata to {METADATA_ARCHIVED_FILE}: {e}")
    except Exception as e:
        logger.error(f"Unexpected error during archived metadata save: {e}", exc_info=True)
# --- END: Load/Save for Archived Metadata --- 

async def _update_task_metadata_and_save(task_uuid: UUID, updates: Dict[str, Any]) -> Optional[TaskMetadata]:
    """Loads metadata, updates a specific task, sets last_modified, and saves."""
    all_metadata = await load_metadata()
    task_uuid_str = str(task_uuid)

    if task_uuid_str not in all_metadata:
        logger.error(f"Task {task_uuid_str} not found in metadata for update.")
        return None # Or raise HTTPException(status_code=404, detail="Task not found")

    task_to_update = all_metadata[task_uuid_str]

    # Apply updates
    for key, value in updates.items():
        if hasattr(task_to_update, key):
            setattr(task_to_update, key, value)
        else:
            logger.warning(f"Attempted to update non-existent field '{key}' on task {task_uuid_str}")

    # Update last_modified timestamp
    task_to_update.last_modified = datetime.now(timezone.utc)
    
    # Ensure embed_url is populated if it's a YouTube task and URL exists (in case it was missed or URL was part of updates)
    if task_to_update.platform == Platform.YOUTUBE and task_to_update.url:
        task_to_update = _populate_embed_url(task_to_update) # _populate_embed_url should ideally not save but just return the object

    all_metadata[task_uuid_str] = task_to_update # Put the updated task back
    await save_metadata(all_metadata)
    logger.info(f"Task {task_uuid_str} updated and metadata saved. Last modified: {task_to_update.last_modified}")
    return task_to_update

async def _download_thumbnail(task_uuid_str: str, thumbnail_url: str, task_data_dir: Path) -> Optional[str]:
    """Downloads a thumbnail image and saves it, returning its relative path."""
    if not thumbnail_url:
        return None
    try:
        # Determine file extension
        # Basic check, can be improved with mimetypes if URL doesn't have extension
        file_ext = Path(thumbnail_url).suffix or '.jpg' # Default to .jpg if no extension
        if not file_ext.startswith('.'): # Ensure it has a dot
             file_ext = '.' + file_ext
        if len(file_ext) > 5: # basic sanity check for extension length
            file_ext = '.jpg' 

        thumbnail_filename = f"thumbnail{file_ext}"
        thumbnail_abs_path = task_data_dir / thumbnail_filename
        
        async with httpx.AsyncClient() as client:
            response = await client.get(thumbnail_url, timeout=10.0) # Added timeout
            response.raise_for_status() # Raise an exception for bad status codes
            async with aiofiles.open(thumbnail_abs_path, 'wb') as f:
                await f.write(response.content)
        
        # Return path relative to DATA_DIR
        return str(Path(task_uuid_str) / thumbnail_filename)
    except httpx.HTTPStatusError as e:
        logger.error(f"HTTP error downloading thumbnail {thumbnail_url} for task {task_uuid_str}: {e.response.status_code} - {e.response.text}")
        return None
    except httpx.RequestError as e:
        logger.error(f"Request error downloading thumbnail {thumbnail_url} for task {task_uuid_str}: {e}")
        return None
    except Exception as e:
        logger.error(f"Error downloading thumbnail {thumbnail_url} for task {task_uuid_str}: {e}", exc_info=True)
        return None

@app.post("/api/ingest", response_model=IngestResponse)
async def ingest_url(request: IngestRequest, background_tasks: BackgroundTasks): # Added BackgroundTasks for potential future use
    try:
        all_metadata = await load_metadata()
        request_url_str = str(request.url)

        for existing_uuid, existing_meta in all_metadata.items():
            if existing_meta.url == request_url_str:
                logger.info(f"URL {request_url_str} already ingested with UUID {existing_uuid}.")
                # Ensure all fields are populated for existing task before returning
                if not existing_meta.info_json_path or not existing_meta.title or not existing_meta.thumbnail_path:
                    logger.info(f"Existing task {existing_uuid} missing some info, attempting to fetch.")
                    try:
                        # Use DATA_DIR as base for run_fetch_info_json
                        info_json_rel_path = await run_fetch_info_json(existing_meta, str(DATA_DIR))
                        existing_meta.info_json_path = info_json_rel_path
                        info_json_abs_path = DATA_DIR / info_json_rel_path
                        if info_json_abs_path.exists():
                            async with aiofiles.open(info_json_abs_path, 'r') as f_json:
                                info_data = json.loads(await f_json.read())
                            existing_meta.title = info_data.get('title')
                            thumbnail_url = info_data.get('thumbnail')
                            if thumbnail_url:
                                task_data_dir = DATA_DIR / str(existing_meta.uuid)
                                existing_meta.thumbnail_path = await _download_thumbnail(str(existing_meta.uuid), thumbnail_url, task_data_dir)
                        existing_meta = _populate_embed_url(existing_meta)
                        existing_meta.last_modified = datetime.now(timezone.utc)
                        all_metadata[existing_uuid] = existing_meta
                        await save_metadata(all_metadata)
                        logger.info(f"Successfully updated missing info for existing task {existing_uuid}.")
                    except Exception as e_fetch:
                        logger.error(f"Failed to fetch missing info for existing task {existing_uuid}: {e_fetch}")
                return IngestResponse(metadata=existing_meta)

        new_uuid = uuid.uuid4()
        task_uuid_str = str(new_uuid)
        task_data_dir = DATA_DIR / task_uuid_str
        task_data_dir.mkdir(parents=True, exist_ok=True)

        platform = Platform.OTHER
        if "youtube.com" in request_url_str or "youtu.be" in request_url_str: platform = Platform.YOUTUBE
        elif "xiaoyuzhoufm.com" in request_url_str: platform = Platform.XIAOYUZHOU

        now = datetime.now(timezone.utc)
        new_task_metadata = TaskMetadata(
            uuid=new_uuid,
            url=request_url_str,
            platform=platform,
            created_at=now,
            last_modified=now
        )

        # Fetch info.json, title, and thumbnail
        try:
            # Use DATA_DIR as base for run_fetch_info_json
            info_json_rel_path = await run_fetch_info_json(new_task_metadata, str(DATA_DIR))
            new_task_metadata.info_json_path = info_json_rel_path
            
            # Read info.json to get title and thumbnail URL
            if info_json_rel_path:
                info_json_abs_path = DATA_DIR / info_json_rel_path
                if info_json_abs_path.exists():
                    async with aiofiles.open(info_json_abs_path, 'r') as f_json:
                        info_data = json.loads(await f_json.read())
                    new_task_metadata.title = info_data.get('title')
                    thumbnail_url = info_data.get('thumbnail') # yt-dlp usually provides 'thumbnail' key for the best quality
                    
                    # Download thumbnail
                    if thumbnail_url:
                        new_task_metadata.thumbnail_path = await _download_thumbnail(task_uuid_str, thumbnail_url, task_data_dir)
                else:
                    logger.warning(f"info.json path provided ({info_json_rel_path}) but file does not exist at {info_json_abs_path}.")            
        except Exception as e_fetch:
            logger.error(f"Failed to fetch info.json or thumbnail for new task {new_uuid}: {e_fetch}", exc_info=True)
            # Continue with partial metadata if fetching info fails, or raise error depending on desired behavior

        new_task_metadata = _populate_embed_url(new_task_metadata)

        all_metadata[task_uuid_str] = new_task_metadata
        await save_metadata(all_metadata)
        logger.info(f"New URL {request_url_str} ingested with UUID {new_uuid}. Title: {new_task_metadata.title}")
        
        return IngestResponse(metadata=new_task_metadata)
    except HTTPException as e: # Specifically re-raise HTTPExceptions
        raise e
    except ValidationError as e:
        logger.error(f"Validation error during ingest: {e}", exc_info=True)
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.error(f"Unexpected error during ingest: {e}", exc_info=True)
        # It's good practice to return a generic error to the client for unexpected issues
        raise HTTPException(status_code=500, detail="Internal server error during ingest process.")

# Add endpoint to list all tasks
@app.get("/api/tasks", response_model=List[TaskMetadata])
async def list_tasks():
    # Load both active and archived metadata
    active_metadata = await load_metadata() # Dict[str, TaskMetadata]
    archived_metadata = await load_archived_metadata() # Dict[str, Dict]
    
    # Create a set of archived UUIDs for efficient lookup
    archived_uuids = set(archived_metadata.keys())
    
    tasks_to_return = []
    
    # Iterate through tasks currently in the active metadata file
    for uuid_str, task_meta in active_metadata.items():
        # Check if this task's UUID is in the set of archived UUIDs
        is_archived = uuid_str in archived_uuids
        
        # Update the archived status on the TaskMetadata object
        task_meta.archived = is_archived
        
        # --- 调用 helper 来填充 embed_url ---
        task_meta = _populate_embed_url(task_meta) 
        # ------------------------------------
        
        tasks_to_return.append(task_meta)

    # Return the list of TaskMetadata objects, now with correct archived status
    return tasks_to_return

# Add endpoint to get a specific task's metadata
@app.get("/api/tasks/{task_uuid}", response_model=TaskMetadata)
async def get_task(task_uuid: UUID):
    metadata = await load_metadata()
    task_meta = metadata.get(str(task_uuid))
    if not task_meta:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # --- 调用 helper 来填充 embed_url ---
    task_meta = _populate_embed_url(task_meta)
    # ------------------------------------

    return task_meta

# Add endpoint to delete a specific task
@app.delete("/api/tasks/{task_uuid}", status_code=204)
async def delete_task(task_uuid: UUID):
    all_metadata = await load_metadata()
    task_uuid_str = str(task_uuid)

    if task_uuid_str not in all_metadata:
        raise HTTPException(status_code=404, detail="Task not found")

    del all_metadata[task_uuid_str]
    await save_metadata(all_metadata)

    # Delete the task's data directory using pathlib
    task_dir = BASE_DIR / task_uuid_str # Use pathlib
    if task_dir.exists() and task_dir.is_dir(): # Use pathlib methods
        try:
            shutil.rmtree(task_dir) # shutil works with Path objects
            logger.info(f"Successfully deleted directory: {task_dir}")
        except OSError as e:
            logger.error(f"Error deleting directory {task_dir}: {e}")
    else:
        logger.warning(f"Task directory not found or is not a directory: {task_dir}")

    return

# Placeholder for future task endpoints
@app.post("/api/tasks/{task_uuid}/fetch_info_json", response_model=FetchInfoJsonResponse)
async def fetch_info_json_endpoint(task_uuid: UUID):
    all_metadata = await load_metadata()
    task_meta = all_metadata.get(str(task_uuid))

    if not task_meta:
        raise HTTPException(status_code=404, detail="Task not found")
        
    # Check if info.json exists (relative to DATA_DIR)
    info_json_abs_path = None
    if task_meta.info_json_path:
        info_json_abs_path = DATA_DIR / task_meta.info_json_path # Corrected base

    if info_json_abs_path and info_json_abs_path.exists():
        return FetchInfoJsonResponse(
            task_uuid=task_uuid,
            info_json_path=task_meta.info_json_path,
            message="info.json already exists."
        )

    try:
        # run_fetch_info_json needs to return path relative to DATA_DIR
        # Assuming run_fetch_info_json internally uses DATA_DIR correctly
        info_json_rel_path = await run_fetch_info_json(task_meta, str(DATA_DIR)) # Pass DATA_DIR as base
        task_meta.info_json_path = info_json_rel_path
        all_metadata[str(task_uuid)] = task_meta
        await save_metadata(all_metadata)
        return FetchInfoJsonResponse(
            task_uuid=task_uuid,
            info_json_path=info_json_rel_path,
            message="info.json downloaded successfully."
        )
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except yt_dlp.utils.DownloadError as e:
        raise HTTPException(status_code=500, detail=f"Failed to download info.json: {e}")
    except Exception as e:
        logger.error(f"Error in fetch_info_json endpoint for {task_uuid}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error during fetch_info_json: {str(e)}")

@app.post("/api/tasks/{task_uuid}/download_vtt", status_code=200)
async def download_vtt_endpoint(task_uuid: UUID):
    task_uuid_str = str(task_uuid)
    logger.info(f"Received request to download VTT for task: {task_uuid_str}")
    
    all_metadata = await load_metadata()
    task_meta = all_metadata.get(task_uuid_str)

    if not task_meta:
        logger.warning(f"Task not found for VTT download: {task_uuid_str}")
        raise HTTPException(status_code=404, detail="Task not found")

    if task_meta.platform != Platform.YOUTUBE:
         logger.warning(f"Attempted VTT download for non-YouTube task: {task_uuid_str} (Platform: {task_meta.platform})")
         raise HTTPException(status_code=400, detail="VTT download is only supported for YouTube videos.")

    # Get the TaskMetadata object needed by the async download function
    task_meta_obj = all_metadata.get(task_uuid_str)
    if not task_meta_obj:
        logger.warning(f"Task not found for VTT download: {task_uuid_str}")
        raise HTTPException(status_code=404, detail="Task not found")

    try:
        # Directly await the async download function
        downloaded_vtt_files = await download_youtube_vtt(task_meta_obj, str(METADATA_FILE))
        
        # Check the result returned by the function
        if downloaded_vtt_files:
            logger.info(f"Successfully downloaded VTT files for task: {task_uuid_str}. Files: {downloaded_vtt_files}")
            return {"message": "VTT download successful.", "vtt_files": downloaded_vtt_files}
        else:
            # Function returned empty dict, likely due to yt-dlp error or no subs found
            logger.warning(f"VTT download process completed for {task_uuid_str}, but no files were downloaded or reported. Check logs.")
            # Return success but indicate no files were generated/found
            return {"message": "VTT download process completed, but no subtitles were downloaded. Check logs for details.", "vtt_files": {}}

    except FileNotFoundError as e:
         logger.error(f"File not found during VTT download process for {task_uuid_str}: {e}", exc_info=True)
         raise HTTPException(status_code=404, detail=f"Required file not found: {e}")
    except Exception as e:
        logger.error(f"Error during VTT download endpoint for {task_uuid_str}: {e}", exc_info=True)
        # Avoid leaking internal details unless necessary
        raise HTTPException(status_code=500, detail=f"Internal server error during VTT download.")

@app.post("/api/tasks/{task_uuid}/download_media", response_model=DownloadMediaResponse)
async def download_media_endpoint(task_uuid: UUID, request: DownloadMediaRequest):
    all_metadata = await load_metadata()
    task_meta = all_metadata.get(str(task_uuid))

    if not task_meta:
        raise HTTPException(status_code=404, detail="Task not found")

    quality = request.quality
    try:
        # Pass BASE_DIR as string if task expects string
        media_rel_path = await run_download_media(task_meta, quality, str(BASE_DIR))
        if task_meta.media_files is None:
            task_meta.media_files = {}
        task_meta.media_files[quality] = media_rel_path
        all_metadata[str(task_uuid)] = task_meta
        await save_metadata(all_metadata)
        return DownloadMediaResponse(
            task_uuid=task_uuid,
            quality=quality,
            media_path=media_rel_path,
            message=f"Media ({quality}) downloaded successfully."
        )
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except yt_dlp.utils.DownloadError as e:
        error_detail = f"Failed to download media ({quality}): {str(e)}"
        if "Unsupported URL" in str(e):
             error_detail = f"Download failed: Unsupported URL ({task_meta.url}) for media download."
        elif "format not available" in str(e).lower():
             error_detail = f"Download failed: Quality '{quality}' not available for this media."
        
        raise HTTPException(status_code=500, detail=error_detail)
    except Exception as e:
        logger.error(f"Error in download_media endpoint for {task_uuid} (Quality: {quality}): {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error during media download: {str(e)}")

@app.post("/api/tasks/{task_uuid}/extract_audio", response_model=ExtractAudioResponse)
async def extract_audio_endpoint(task_uuid: UUID):
    all_metadata = await load_metadata()
    task_meta = all_metadata.get(str(task_uuid))

    if not task_meta:
        raise HTTPException(status_code=404, detail="Task not found")

    if not task_meta.media_files or not any(task_meta.media_files.values()): # Check if any video path exists
        raise HTTPException(
            status_code=400, 
            detail="Cannot extract audio: No media files found in metadata. Please download video/audio first."
        )
        
    # Check if already extracted (relative to DATA_DIR)
    extracted_wav_abs_path = None
    if task_meta.extracted_wav_path:
        extracted_wav_abs_path = DATA_DIR / task_meta.extracted_wav_path # Corrected base
        
    if extracted_wav_abs_path and extracted_wav_abs_path.exists(): # Use pathlib
        logger.info(f"Audio already extracted for task {task_uuid}")
        return ExtractAudioResponse(
            task_uuid=task_uuid,
            wav_path=task_meta.extracted_wav_path,
            message="Audio already extracted."
        )

    try:
        logger.info(f"Starting audio extraction for task {task_uuid}")
        # Pass BASE_DIR as string if task expects string
        extracted_wav_rel_path = await run_extract_audio(task_meta, str(BACKEND_DIR), str(DATA_DIR))
        task_meta.extracted_wav_path = extracted_wav_rel_path
        all_metadata[str(task_uuid)] = task_meta
        await save_metadata(all_metadata)
        logger.info(f"Successfully extracted audio for task {task_uuid} to {extracted_wav_rel_path}")
        return ExtractAudioResponse(
            task_uuid=task_uuid,
            wav_path=extracted_wav_rel_path,
            message="Audio extracted successfully."
        )
    except FileNotFoundError as e:
        logger.error(f"Audio extraction failed for {task_uuid}: Input file not found - {e}")
        raise HTTPException(status_code=404, detail=str(e))
    except ValueError as e:
        logger.error(f"Audio extraction failed for {task_uuid}: Value error - {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except ffmpeg.Error as e:
        logger.error(f"Audio extraction failed for {task_uuid} due to ffmpeg error: {e}")
        raise HTTPException(status_code=500, detail=f"ffmpeg error during audio extraction: {e}")
    except Exception as e:
        logger.error(f"Unexpected error during audio extraction for {task_uuid}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error during audio extraction: {str(e)}")

@app.post("/api/tasks/{task_uuid}/merge", response_model=MergeResponse)
async def merge_transcripts_endpoint(task_uuid: UUID):
    all_metadata = await load_metadata()
    task_meta = all_metadata.get(str(task_uuid))

    if not task_meta:
        raise HTTPException(status_code=404, detail="Task not found")

    merged_md_path = None
    source_files = []
    message = ""
    
    # Helper function using pathlib
    def check_rel_path_exists_in_data(rel_path):
        if not rel_path:
            return False
        # Check existence relative to DATA_DIR
        return (DATA_DIR / rel_path).exists()

    try:
        if task_meta.platform == Platform.YOUTUBE:
            logger.info(f"Starting VTT merge for YouTube task {task_uuid}")
            if task_meta.merged_vtt_md_path and check_rel_path_exists_in_data(task_meta.merged_vtt_md_path):
                logger.info(f"Merged VTT markdown already exists for {task_uuid}")
                merged_md_path = task_meta.merged_vtt_md_path
                source_files = [str(DATA_DIR / p) for p in task_meta.vtt_files.values() if p and check_rel_path_exists_in_data(p)]
                message = "Merged VTT markdown file already exists."
            else:
                if not task_meta.vtt_files:
                     raise HTTPException(status_code=400, detail="Cannot merge: No VTT files found in metadata. Run VTT transcription first.")
                vtt_paths_exist = [ check_rel_path_exists_in_data(p) for p in task_meta.vtt_files.values() if p ]
                if not any(vtt_paths_exist):
                    raise HTTPException(status_code=400, detail="Cannot merge: Source VTT files listed in metadata not found on disk.")
                
                # Pass BASE_DIR as string
                merged_md_path, source_files_abs = await run_merge_vtt(task_meta, str(DATA_DIR), str(DATA_DIR))
                task_meta.merged_vtt_md_path = merged_md_path # Assuming task returns relative path
                message = f"VTT transcripts merged successfully into markdown."
                logger.info(message)
                all_metadata[str(task_uuid)] = task_meta
                await save_metadata(all_metadata)
                # Get filenames from absolute paths returned by task
                source_files = [os.path.basename(p) for p in source_files_abs]
        
        else: # Non-YouTube platforms merge WhisperX JSON
            logger.info(f"Starting WhisperX merge for task {task_uuid}")
            if task_meta.merged_whisperx_md_path and check_rel_path_exists_in_data(task_meta.merged_whisperx_md_path):
                 logger.info(f"Merged WhisperX markdown already exists for {task_uuid}")
                 merged_md_path = task_meta.merged_whisperx_md_path
                 source_files = [os.path.basename(os.path.join(DATA_DIR, task_meta.whisperx_json_path))] if task_meta.whisperx_json_path else []
                 message = "Merged WhisperX markdown file already exists."
            else:
                if not task_meta.whisperx_json_path or not check_rel_path_exists_in_data(task_meta.whisperx_json_path):
                    raise HTTPException(status_code=400, detail="Cannot merge: WhisperX JSON file not found. Run WhisperX transcription first.")

                # Pass BASE_DIR as string
                merged_md_path, source_files_abs = await run_merge_whisperx(task_meta, str(DATA_DIR), str(DATA_DIR))
                task_meta.merged_whisperx_md_path = merged_md_path # Assuming task returns relative path
                message = f"WhisperX transcript merged successfully into markdown."
                logger.info(message)
                all_metadata[str(task_uuid)] = task_meta
                await save_metadata(all_metadata)
                # Get filenames from absolute paths returned by task
                source_files = [os.path.basename(p) for p in source_files_abs]

        return MergeResponse(
            task_uuid=task_uuid,
            merged_file_path=merged_md_path,
            source_files=source_files, 
            message=message
        )

    except (FileNotFoundError, ValueError) as e:
        logger.error(f"Merge error for {task_uuid}: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Unexpected error during merge for {task_uuid}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error during merge: {str(e)}")

# <<< Add New Endpoints Here >>>
@app.delete("/api/tasks/{task_uuid}/media/video", status_code=204)
async def delete_task_video_files(task_uuid: UUID):
    """
    Deletes video files associated with a task and updates metadata.
    """
    all_metadata = await load_metadata()
    task_uuid_str = str(task_uuid)

    if task_uuid_str not in all_metadata:
        raise HTTPException(status_code=404, detail="Task not found")

    task_meta = all_metadata[task_uuid_str]
    task_meta_dict = task_meta.dict() # Work with dict for sync function

    try:
        # Pass DATA_DIR to the sync function
        updated_task_meta_dict = await run_in_threadpool(
            delete_video_files_sync, 
            uuid=task_uuid_str, 
            entry=task_meta_dict, 
            data_dir=str(DATA_DIR) # Pass DATA_DIR
        )
        
        # Update the specific field in the Pydantic model
        if 'media_files' in updated_task_meta_dict:
            task_meta.media_files = updated_task_meta_dict['media_files']
        else:
             task_meta.media_files = {}

        all_metadata[task_uuid_str] = task_meta
        await save_metadata(all_metadata)
        
        logger.info(f"Successfully processed video files for task {task_uuid_str}") # Adjusted log msg
        return

    except Exception as e:
        logger.error(f"Error processing video files for task {task_uuid_str}: {e}", exc_info=True) # Adjusted log msg
        raise HTTPException(status_code=500, detail=f"Failed to process video files: {str(e)}")

@app.delete("/api/tasks/{task_uuid}/media/audio", status_code=204)
async def delete_task_audio_file(task_uuid: UUID):
    """
    Deletes the audio file associated with a task and updates metadata.
    """
    all_metadata = await load_metadata()
    task_uuid_str = str(task_uuid)

    if task_uuid_str not in all_metadata:
        raise HTTPException(status_code=404, detail="Task not found")

    task_meta = all_metadata[task_uuid_str]
    task_meta_dict = task_meta.dict() # Work with dict for sync function

    try:
        # Pass DATA_DIR to the sync function
        updated_task_meta_dict = await run_in_threadpool(
            delete_audio_file_sync, 
            uuid=task_uuid_str, 
            entry=task_meta_dict, 
            data_dir=str(DATA_DIR) # Pass DATA_DIR
        )
        
        # Update the specific field in the Pydantic model
        task_meta.extracted_wav_path = updated_task_meta_dict.get('extracted_wav_path') 

        all_metadata[task_uuid_str] = task_meta
        await save_metadata(all_metadata)
        
        logger.info(f"Successfully processed audio file for task {task_uuid_str}") # Adjusted log msg
        return

    except Exception as e:
        logger.error(f"Error processing audio file for task {task_uuid_str}: {e}", exc_info=True) # Adjusted log msg
        raise HTTPException(status_code=500, detail=f"Failed to process audio file: {str(e)}")

# --- ADD NEW DELETE VTT ENDPOINT HERE ---
@app.delete("/api/tasks/{task_uuid}/vtt/{lang_code}", status_code=204)
async def delete_vtt_file(task_uuid: UUID, lang_code: str):
    task_uuid_str = str(task_uuid)
    logger.info(f"Received request to delete VTT file ({lang_code}) for task: {task_uuid_str}")

    all_metadata = await load_metadata()
    task_meta = all_metadata.get(task_uuid_str)

    if not task_meta:
        logger.warning(f"Task not found for VTT deletion: {task_uuid_str}")
        raise HTTPException(status_code=404, detail="Task not found")

    if not task_meta.vtt_files or lang_code not in task_meta.vtt_files:
        logger.warning(f"VTT file for language '{lang_code}' not found in metadata for task {task_uuid_str}")
        # Return 404, maybe the file was already deleted or never existed
        raise HTTPException(status_code=404, detail=f"VTT file for language '{lang_code}' not found")

    vtt_rel_path_str = task_meta.vtt_files.get(lang_code)
    if not vtt_rel_path_str:
         # This case should be covered by the check above, but belts and suspenders
         raise HTTPException(status_code=404, detail=f"VTT path for language '{lang_code}' is empty or invalid in metadata")

    vtt_abs_path = DATA_DIR / vtt_rel_path_str # Use pathlib

    file_deleted = False
    # Attempt to delete the file from filesystem
    try:
        if vtt_abs_path.exists() and vtt_abs_path.is_file():
            # Run blocking os.remove in threadpool
            await run_in_threadpool(os.remove, vtt_abs_path)
            logger.info(f"Successfully deleted VTT file from disk: {vtt_abs_path}")
            file_deleted = True
        else:
            logger.warning(f"VTT file not found on disk at expected path: {vtt_abs_path}. Proceeding to update metadata only.")
            # Consider if this should be an error, but often we just want to clean metadata
            file_deleted = False # Explicitly false, though already initialized

    except OSError as e:
        logger.error(f"Error deleting VTT file from disk {vtt_abs_path}: {e}")
        # Decide if we should stop or just log and continue to update metadata
        # For now, let's raise 500 as failure to delete is significant
        raise HTTPException(status_code=500, detail=f"Error deleting VTT file from disk: {e}")

    # Update metadata: remove the entry for the deleted language
    try:
        del task_meta.vtt_files[lang_code]
        all_metadata[task_uuid_str] = task_meta # Put the modified task_meta back
        await save_metadata(all_metadata)
        logger.info(f"Successfully removed '{lang_code}' VTT entry from metadata for task {task_uuid_str}")
    except Exception as e:
        logger.error(f"Failed to update metadata after VTT file deletion for task {task_uuid_str}: {e}", exc_info=True)
        # If file deletion succeeded but metadata update failed, this is problematic.
        # Raise 500 to indicate an inconsistent state.
        raise HTTPException(status_code=500, detail="Failed to update metadata after file operation.")

    # Return 204 No Content on success
    return

# --- START: New GET VTT Endpoint ---
@app.get("/api/tasks/{task_uuid}/vtt/{lang_code}", response_class=FileResponse)
async def get_vtt_file(task_uuid: UUID, lang_code: str):
    task_uuid_str = str(task_uuid)
    logger.info(f"Received request to get VTT file ({lang_code}) for task: {task_uuid_str}")

    all_metadata = await load_metadata()
    task_meta = all_metadata.get(task_uuid_str)

    if not task_meta:
        logger.warning(f"Task not found for VTT get: {task_uuid_str}")
        raise HTTPException(status_code=404, detail="Task not found")

    if not task_meta.vtt_files or lang_code not in task_meta.vtt_files:
        logger.warning(f"VTT file for language '{lang_code}' not found in metadata for task {task_uuid_str}")
        raise HTTPException(status_code=404, detail=f"VTT file for language '{lang_code}' not found")

    vtt_rel_path_str = task_meta.vtt_files.get(lang_code)
    if not vtt_rel_path_str:
         raise HTTPException(status_code=404, detail=f"VTT path for language '{lang_code}' is empty or invalid in metadata")

    vtt_abs_path = DATA_DIR / vtt_rel_path_str

    if not vtt_abs_path.exists() or not vtt_abs_path.is_file():
        logger.error(f"VTT file specified in metadata not found on disk: {vtt_abs_path}")
        raise HTTPException(status_code=404, detail=f"VTT file not found on server at path: {vtt_rel_path_str}")

    # Return the file as a response
    # Use the original filename stored in metadata if available, otherwise construct it
    # The relative path usually contains the filename.
    filename = Path(vtt_rel_path_str).name 
    return FileResponse(path=vtt_abs_path, filename=filename, media_type='text/vtt')
# --- END: New GET VTT Endpoint ---


# --- START: New GET Markdown Endpoint ---
@app.get("/api/tasks/{task_uuid}/markdown/{format}", response_class=FileResponse)
async def get_markdown_file(task_uuid: UUID, format: Literal['merged', 'parallel']):
    task_uuid_str = str(task_uuid)
    logger.info(f"Received request to get Markdown file (format: {format}) for task: {task_uuid_str}")

    all_metadata = await load_metadata()
    task_meta = all_metadata.get(task_uuid_str)

    if not task_meta:
        logger.warning(f"Task not found for Markdown get: {task_uuid_str}")
        raise HTTPException(status_code=404, detail="Task not found")

    markdown_rel_path_str = None
    if format == 'merged':
        markdown_rel_path_str = task_meta.merged_format_vtt_md_path
    elif format == 'parallel':
        markdown_rel_path_str = task_meta.parallel_vtt_md_path
    
    if not markdown_rel_path_str:
        logger.warning(f"Markdown file (format: {format}) path not found in metadata for task {task_uuid_str}")
        raise HTTPException(status_code=404, detail=f"Markdown transcript (format: {format}) not found in metadata. Has it been generated?")

    markdown_abs_path = DATA_DIR / markdown_rel_path_str

    if not markdown_abs_path.exists() or not markdown_abs_path.is_file():
        logger.error(f"Markdown file specified in metadata not found on disk: {markdown_abs_path}")
        raise HTTPException(status_code=404, detail=f"Markdown file not found on server at path: {markdown_rel_path_str}")

    # Return the file as a response
    filename = Path(markdown_rel_path_str).name
    return FileResponse(path=markdown_abs_path, filename=filename, media_type='text/markdown')
# --- END: New GET Markdown Endpoint ---

# --- START: New List Files Endpoint ---
@app.get("/api/tasks/{task_uuid}/files/list", response_model=List[str])
async def list_task_files(
    task_uuid: UUID,
    extension: Optional[str] = Query(None, description="Filter by file extension (e.g., .txt, .md)")
):
    """
    Lists files within the specified task's data directory, optionally filtering by extension.
    Only lists files, not directories.
    """
    task_uuid_str = str(task_uuid)
    logger.info(f"Request to list files for task {task_uuid_str} (extension filter: {extension})")

    task_data_dir = DATA_DIR / task_uuid_str

    if not task_data_dir.exists() or not task_data_dir.is_dir():
        logger.warning(f"Task data directory not found: {task_data_dir}")
        raise HTTPException(status_code=404, detail="Task data directory not found")

    try:
        all_files = []
        for item in task_data_dir.iterdir():
            if item.is_file():
                # Apply extension filter if provided
                if extension:
                    # Ensure extension starts with a dot for consistent comparison
                    filter_ext = extension if extension.startswith('.') else f".{extension}"
                    if item.name.lower().endswith(filter_ext.lower()):
                        all_files.append(item.name)
                else:
                    # No filter, add all files
                    all_files.append(item.name)
        
        logger.info(f"Found {len(all_files)} files for task {task_uuid_str} matching filter '{extension}'")
        return sorted(all_files) # Return sorted list

    except Exception as e:
        logger.error(f"Error listing files in directory {task_data_dir}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error listing files")
# --- END: New List Files Endpoint ---

# --- START: 新增 - 获取任务目录下的任意文件 ---
# Use api_route to allow both GET and HEAD for FileResponse
@app.api_route("/api/tasks/{task_uuid}/files/{filename}", methods=["GET", "HEAD"], response_class=FileResponse)
async def get_task_file(task_uuid: UUID, filename: str):
    """
    提供任务数据目录中指定文件的访问。
    进行了基本的安全检查，防止访问目录外的文件。
    支持子目录中的文件，例如 "markdown/file.md"。
    """
    task_uuid_str = str(task_uuid)
    logger.info(f"请求获取任务 {task_uuid_str} 的文件: {filename}")

    # 基础安全检查：防止文件名包含路径遍历字符
    if ".." in filename or filename.startswith("/"):
        logger.warning(f"检测到非法的文件名请求: {filename} (任务: {task_uuid_str})")
        raise HTTPException(status_code=400, detail="不允许的文件名")

    # 构建文件的绝对路径 - 支持子目录
    file_path_to_serve = DATA_DIR / task_uuid_str / filename
    
    # --- 重要的安全检查：确保文件路径在预期的 DATA_DIR/{task_uuid} 目录下 ---
    resolved_file_path = None
    expected_task_dir = None
    try:
        logger.debug(f"Attempting to resolve expected_task_dir: {DATA_DIR / task_uuid_str}")
        expected_task_dir = (DATA_DIR / task_uuid_str).resolve(strict=True) 
        logger.debug(f"Successfully resolved expected_task_dir: {expected_task_dir}")
        
        logger.debug(f"Attempting to resolve file_path_to_serve: {file_path_to_serve}")
        resolved_file_path = await run_in_threadpool(file_path_to_serve.resolve, strict=True) 
        logger.debug(f"Successfully resolved file_path_to_serve: {resolved_file_path}")

        if not str(resolved_file_path).startswith(str(expected_task_dir)):
             logger.warning(f"Path Traversal Check Failed: Resolved path {resolved_file_path} does not start with expected directory {expected_task_dir}")
             raise HTTPException(status_code=404, detail="文件未找到或不在允许的目录下")
        else:
            logger.debug("Path Traversal Check Passed.")

    except FileNotFoundError as fnf_error:
        logger.warning(f"FileNotFoundError during path resolution for {file_path_to_serve}. Error: {fnf_error}")
        logger.warning(f"  Attempted to resolve expected_task_dir: {DATA_DIR / task_uuid_str} -> Result: {expected_task_dir}")
        logger.warning(f"  Attempted to resolve file_path_to_serve: {file_path_to_serve} -> Result: {resolved_file_path}") 
        raise HTTPException(status_code=404, detail=f"文件未找到(路径解析失败): {filename}")
    except Exception as e: 
         logger.error(f"检查文件路径时出错: {file_path_to_serve} - {e}", exc_info=True)
         raise HTTPException(status_code=500, detail="服务器内部错误")
    # -----------------------------------------------------------------------

    # 检查路径是否确实是一个文件
    logger.debug(f"Checking if path is a file: {file_path_to_serve}")
    is_a_file = False
    try:
        is_a_file = await run_in_threadpool(file_path_to_serve.is_file) 
        logger.debug(f"Path.is_file() result: {is_a_file}")
    except Exception as e:
        logger.error(f"Error calling is_file() on {file_path_to_serve}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="检查文件类型时出错")
        
    if not is_a_file:
        logger.warning(f"Path is not a file: {file_path_to_serve}")
        raise HTTPException(status_code=404, detail=f"请求的路径不是一个文件: {filename}")

    # 猜测文件的 MIME 类型
    logger.debug(f"Guessing media type for: {file_path_to_serve}")
    media_type, _ = mimetypes.guess_type(file_path_to_serve)
    if media_type is None:
        media_type = 'application/octet-stream' 
    # 对于 .vtt 文件，强制使用 text/vtt
    if filename.lower().endswith('.vtt'):
        media_type = 'text/vtt'
    # 对于 .md 文件，强制使用 text/markdown
    elif filename.lower().endswith('.md'):
        media_type = 'text/markdown'

    logger.info(f"正在提供文件: {file_path_to_serve} (类型: {media_type})，请求名: {filename}")
    return FileResponse(path=str(file_path_to_serve), filename=os.path.basename(filename), media_type=media_type)
# --- END: 新增 - 获取任务目录下的任意文件 ---


# --- Restore Archived Endpoint --- 
@app.post("/api/tasks/restore_archived", status_code=200)
async def restore_archived_tasks_endpoint():
    logger.info("Received request to restore archived tasks by re-ingesting URLs.")
    
    restored_tasks = []
    skipped_tasks = []
    failed_tasks = []
    metadata_changed = False

    try:
        # Load both sets of metadata
        archived_data = await load_archived_metadata() # Dict[str, Dict]
        active_metadata = await load_metadata()       # Dict[str, TaskMetadata]

        if not archived_data:
            logger.info("No archived metadata found to restore.")
            return {"message": "No archived tasks found to restore.", "restored": [], "skipped": [], "failed": []}

        # Create a set of active URLs for quick lookup
        active_urls = {meta.url for meta in active_metadata.values()}
        logger.debug(f"Found {len(active_urls)} active URLs.")

        for archived_uuid, archived_item in archived_data.items():
            archived_url = archived_item.get('url')
            if not archived_url:
                logger.warning(f"Archived item {archived_uuid} has no URL. Skipping.")
                failed_tasks.append({"uuid": archived_uuid, "reason": "Missing URL"})
                continue

            # Check if URL is already active
            if archived_url in active_urls:
                logger.info(f"URL {archived_url} (from archived UUID {archived_uuid}) is already active. Skipping restore.")
                skipped_tasks.append({"uuid": archived_uuid, "url": archived_url})
                continue

            # Attempt to re-ingest the URL (create task, download thumb)
            logger.info(f"Attempting to re-ingest URL: {archived_url} (from archived UUID {archived_uuid})")
            new_task_meta: Optional[TaskMetadata] = None
            try:
                # --- Step 1: Create basic task entry and download thumbnail --- 
                new_task_meta = await create_ingest_task(archived_url, str(BASE_DIR))
                logger.info(f"Successfully created task entry for URL {archived_url} with new UUID {new_task_meta.uuid}")

            except (ValueError, HTTPException) as ingest_err:
                logger.error(f"Failed initial ingest for URL {archived_url}: {ingest_err}")
                failed_tasks.append({"uuid": archived_uuid, "url": archived_url, "reason": f"Initial ingest failed: {ingest_err}"})
                continue # Skip to next URL if initial ingest fails
            except Exception as e:
                logger.error(f"Unexpected error during initial ingest for URL {archived_url}: {e}", exc_info=True)
                failed_tasks.append({"uuid": archived_uuid, "url": archived_url, "reason": f"Unexpected initial ingest error: {e}"})
                continue # Skip to next URL

            # --- Step 2: Fetch info.json for the newly created task --- 
            if new_task_meta: # Proceed only if initial ingest was successful
                try:
                    logger.info(f"Attempting to fetch info.json for new task {new_task_meta.uuid}...")
                    # Call the function that handles info.json download
                    info_json_rel_path = await run_fetch_info_json(new_task_meta, str(DATA_DIR))
                    
                    # Update the metadata object with the path
                    if info_json_rel_path:
                        new_task_meta.info_json_path = info_json_rel_path
                        logger.info(f"Successfully fetched info.json for {new_task_meta.uuid}. Path: {info_json_rel_path}")
                    else:
                        # Should not happen if run_fetch_info_json succeeded without error, but log just in case
                        logger.warning(f"run_fetch_info_json completed for {new_task_meta.uuid} but returned no path.")
                        # Mark as failure? Or just proceed without the path?
                        # Let's add to failed_tasks for clarity
                        failed_tasks.append({"uuid": str(new_task_meta.uuid), "url": archived_url, "reason": "info.json fetch succeeded but returned no path"})

                except yt_dlp.utils.DownloadError as dl_error:
                    logger.warning(f"Failed to fetch info.json for {new_task_meta.uuid}: {dl_error}")
                    # Add to failed list, but keep the task created in Step 1
                    failed_tasks.append({"uuid": str(new_task_meta.uuid), "url": archived_url, "reason": f"info.json download failed: {dl_error}"})
                except Exception as e:
                    logger.error(f"Unexpected error fetching info.json for {new_task_meta.uuid}: {e}", exc_info=True)
                    # Add to failed list, but keep the task created in Step 1
                    failed_tasks.append({"uuid": str(new_task_meta.uuid), "url": archived_url, "reason": f"Unexpected info.json fetch error: {e}"})

                # --- Step 3: Add/Update the task in active metadata --- 
                # Always add the task to metadata, even if info.json failed
                active_metadata[str(new_task_meta.uuid)] = new_task_meta
                # Add the *new* UUID and URL to the list of restored tasks
                # Note: We add to 'restored' even if info.json fails, as the base task exists
                restored_tasks.append({"new_uuid": str(new_task_meta.uuid), "url": archived_url})
                metadata_changed = True # Mark that we need to save

        # Save metadata ONLY if changes were made
        if metadata_changed:
            logger.info(f"Saving updated metadata after restoring {len(restored_tasks)} tasks.")
            await save_metadata(active_metadata)
        else:
            logger.info("No changes made to active metadata during restore process.")

        # Construct summary message
        message = f"Restore process completed. Restored: {len(restored_tasks)}, Skipped (already active): {len(skipped_tasks)}, Failed: {len(failed_tasks)}."
        logger.info(message)
        return {"message": message, "restored": restored_tasks, "skipped": skipped_tasks, "failed": failed_tasks}

    except FileNotFoundError:
        logger.error(f"Restore failed: Archived metadata file not found at {METADATA_ARCHIVED_FILE}")
        raise HTTPException(status_code=404, detail=f"Archived metadata file not found: {METADATA_ARCHIVED_FILE}")
    except Exception as e:
        logger.error(f"Internal server error during restore of archived tasks: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error during restore process: {str(e)}")

# --- START: New Download Audio Endpoint ---
@app.post("/api/tasks/{task_uuid}/download_audio", response_model=DownloadAudioResponse)
async def download_audio_endpoint(task_uuid: UUID):
    task_uuid_str = str(task_uuid)
    logger.info(f"Received request to download audio for task: {task_uuid_str}")

    all_metadata = await load_metadata()
    task_meta = all_metadata.get(task_uuid_str)

    if not task_meta:
        logger.warning(f"Task not found for audio download: {task_uuid_str}")
        raise HTTPException(status_code=404, detail="Task not found")

    # Check platform suitability
    if task_meta.platform not in AUDIO_DOWNLOAD_PLATFORMS:
        logger.warning(f"Direct audio download not supported for platform '{task_meta.platform}' on task {task_uuid_str}")
        raise HTTPException(status_code=400, detail=f"Direct audio download not supported for platform: {task_meta.platform}")

    # Check if audio already downloaded (relative to DATA_DIR)
    if task_meta.downloaded_audio_path:
         audio_abs_path = DATA_DIR / task_meta.downloaded_audio_path # Corrected base
         if audio_abs_path.exists():
            logger.info(f"Audio already downloaded for task {task_uuid_str} at {task_meta.downloaded_audio_path}")
            return DownloadAudioResponse(
                task_uuid=task_uuid,
                audio_path=task_meta.downloaded_audio_path,
                message="Audio already downloaded."
            )
         else:
             logger.warning(f"Metadata indicates audio downloaded for {task_uuid_str}, but file not found at {audio_abs_path}. Proceeding with download attempt.")
             # Reset path in metadata before attempting download again?
             # task_meta.downloaded_audio_path = None # Maybe? For now, just proceed.

    # Check if info.json exists (relative to DATA_DIR)
    if not task_meta.info_json_path:
         logger.warning(f"Cannot download audio for {task_uuid_str}: info.json path is missing.")
         raise HTTPException(status_code=400, detail="Info JSON has not been fetched yet. Please fetch info first.")
    info_json_abs_path = DATA_DIR / task_meta.info_json_path # Corrected base
    if not info_json_abs_path.exists():
         logger.warning(f"Cannot download audio for {task_uuid_str}: info.json file not found at {info_json_abs_path}.")
         raise HTTPException(status_code=404, detail="Info JSON file not found on disk.")

    try:
        # Run the download function in threadpool
        audio_rel_path = await run_in_threadpool(
            download_audio_sync, 
            task_uuid_str,
            str(METADATA_FILE)
        )

        if not audio_rel_path:
            # This means download_audio_sync failed internally (e.g., no format found, request error)
            logger.error(f"Direct audio download failed for task {task_uuid_str}. Check task logs.")
            raise HTTPException(status_code=500, detail="Audio download failed. Could not find suitable format or download error.")

        # Reload metadata to avoid race condition before saving
        current_metadata = await load_metadata()
        if task_uuid_str in current_metadata:
             current_metadata[task_uuid_str].downloaded_audio_path = audio_rel_path
             await save_metadata(current_metadata)
             logger.info(f"Successfully downloaded audio for {task_uuid_str} and updated metadata.")
             return DownloadAudioResponse(
                task_uuid=task_uuid,
                audio_path=audio_rel_path,
                message="Audio downloaded successfully."
            )
        else:
            # Should not happen if task existed before
            logger.error(f"Task {task_uuid_str} disappeared from metadata during audio download.")
            raise HTTPException(status_code=500, detail="Task metadata inconsistency during audio download.")

    except Exception as e:
        # Catch potential errors from download_audio_sync or other issues
        logger.error(f"Error during audio download endpoint for {task_uuid_str}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error during audio download: {str(e)}")
# --- END: New Download Audio Endpoint ---

# --- NEW: Merge VTT Endpoint ---
class MergeVttRequest(BaseModel):
    format: Literal['parallel', 'merged', 'en_only', 'zh_only', 'all'] = 'parallel' # 更新格式选项
    use_segmented: bool = False  # 是否使用自然断句处理后的VTT文件

@app.post("/api/tasks/{task_uuid}/merge_vtt", status_code=200)
async def merge_vtt_endpoint(task_uuid: UUID, request: MergeVttRequest):
    metadata = await load_metadata()
    task_meta = metadata.get(str(task_uuid))
    task_uuid_str = str(task_uuid)

    if not task_meta:
        raise HTTPException(status_code=404, detail="Task not found")
    if task_meta.archived:
        raise HTTPException(status_code=400, detail="Task is archived")
    if task_meta.platform != Platform.YOUTUBE:
        raise HTTPException(status_code=400, detail="VTT merging is only supported for YouTube platform")
    
    # Define paths for the script and output
    TASKS_DIR = SRC_DIR / 'tasks' 
    script_path = str(TASKS_DIR / "merge_vtt.py")
    task_data_dir = DATA_DIR / task_uuid_str
    
    # 处理 'all' 格式，一次生成所有类型
    if request.format == 'all':
        result_paths = {}
        errors = []
        # 获取 VTT 文件路径
        en_vtt_rel_path = task_meta.vtt_files.get('en')
        zh_vtt_rel_path = task_meta.vtt_files.get('zh-Hans')
        if not en_vtt_rel_path and not zh_vtt_rel_path:
            raise HTTPException(status_code=400, detail="Cannot merge: Neither English nor Chinese VTT file found in metadata.")
        # 依次生成四种格式
        formats = ['merged', 'parallel', 'en_only', 'zh_only', 'en_only_timestamp', 'zh_only_timestamp']
        for fmt in formats:
            output_filename = f"{fmt}_transcript_vtt.md" # Default, can be overridden
            if fmt == 'merged':
                output_filename = "merged_transcript_vtt.md"
            elif fmt == 'parallel':
                output_filename = "parallel_transcript_vtt.md"
            elif fmt == 'en_only':
                output_filename = "en_transcript_vtt.md"
            elif fmt == 'zh_only':
                output_filename = "zh_transcript_vtt.md"
            elif fmt == 'en_only_timestamp':
                output_filename = "en_transcript_vtt_timestamp.md"
            elif fmt == 'zh_only_timestamp':
                output_filename = "zh_transcript_vtt_timestamp.md"
            else:
                errors.append({fmt: "Unknown format for filename generation"})
                continue

            output_abs_path = task_data_dir / output_filename
            en_arg = str(DATA_DIR / en_vtt_rel_path) if en_vtt_rel_path and (DATA_DIR / en_vtt_rel_path).exists() else "MISSING"
            zh_arg = str(DATA_DIR / zh_vtt_rel_path) if zh_vtt_rel_path and (DATA_DIR / zh_vtt_rel_path).exists() else "MISSING"
            command = [ sys.executable, script_path, en_arg, zh_arg, fmt, str(output_abs_path), str(request.use_segmented).lower() ]
            process = await run_in_threadpool(
                subprocess.run,
                command,
                capture_output=True,
                text=True,
                check=False,
                encoding='utf-8'
            )
            if process.returncode == 0:
                result_paths[fmt] = str(Path(task_uuid_str) / output_filename)
                # 更新 metadata
                if fmt == 'merged':
                    task_meta.merged_format_vtt_md_path = str(Path(task_uuid_str) / output_filename)
                elif fmt == 'parallel':
                    task_meta.parallel_vtt_md_path = str(Path(task_uuid_str) / output_filename)
                elif fmt == 'en_only':
                    task_meta.en_only_vtt_md_path = str(Path(task_uuid_str) / output_filename)
                elif fmt == 'zh_only':
                    task_meta.zh_only_vtt_md_path = str(Path(task_uuid_str) / output_filename)
                elif fmt == 'en_only_timestamp':
                    task_meta.en_only_vtt_timestamp_md_path = str(Path(task_uuid_str) / output_filename)
                elif fmt == 'zh_only_timestamp':
                    task_meta.zh_only_vtt_timestamp_md_path = str(Path(task_uuid_str) / output_filename)
            else:
                errors.append({fmt: process.stderr[:200]})
        metadata[task_uuid_str] = task_meta
        await save_metadata(metadata)
        return {"message": "All formats merged", "result_paths": result_paths, "errors": errors}
    # 下面只处理单一格式
    if request.format == 'parallel':
        target_metadata_field = 'parallel_vtt_md_path'
    elif request.format == 'merged':
        target_metadata_field = 'merged_format_vtt_md_path'
    elif request.format == 'en_only':
        target_metadata_field = 'en_only_vtt_md_path'
    elif request.format == 'zh_only':
        target_metadata_field = 'zh_only_vtt_md_path'
    else:
        raise HTTPException(status_code=400, detail=f"Invalid format requested: {request.format}")

    existing_path = getattr(task_meta, target_metadata_field, None)
    if existing_path and (DATA_DIR / existing_path).exists():
        logger.info(f"VTT format '{request.format}' already exists for task {task_uuid_str} at {existing_path}")
        return {
            "message": f"VTT transcript (format: {request.format}) already exists.",
            "merged_file_path": existing_path # Return the existing path
        }

    # Find required VTT file paths
    en_vtt_rel_path = task_meta.vtt_files.get('en')
    zh_vtt_rel_path = task_meta.vtt_files.get('zh-Hans')

    if not en_vtt_rel_path and not zh_vtt_rel_path:
        raise HTTPException(status_code=400, detail="Cannot merge: Neither English nor Chinese VTT file found in metadata.")

    # Construct absolute paths and check file existence
    en_vtt_abs = DATA_DIR / en_vtt_rel_path if en_vtt_rel_path and (DATA_DIR / en_vtt_rel_path).exists() else None
    zh_vtt_abs = DATA_DIR / zh_vtt_rel_path if zh_vtt_rel_path and (DATA_DIR / zh_vtt_rel_path).exists() else None

    # The script needs at least one valid VTT file on disk
    if not en_vtt_abs and not zh_vtt_abs:
         raise HTTPException(status_code=400, detail="VTT file paths found in metadata, but corresponding files not found on disk.")

    # Build the command - Pass absolute paths or "MISSING" sentinel
    # IMPORTANT: Assumes merge_vtt.py is updated to handle 5 args: <en_path|MISSING> <zh_path|MISSING> <format> <output_abs_path>
    en_arg = str(en_vtt_abs) if en_vtt_abs else "MISSING" # Convert Path to string
    zh_arg = str(zh_vtt_abs) if zh_vtt_abs else "MISSING" # Convert Path to string
    command = [ sys.executable, script_path, en_arg, zh_arg, request.format, str(output_abs_path), str(request.use_segmented).lower() ]

    logger.info(f"Running merge script for task {task_uuid_str}: {' '.join(command)}")

    try:
        # Run the script in a threadpool as it involves file I/O
        process = await run_in_threadpool(
            subprocess.run,
            command,
            capture_output=True,
            text=True,
            check=False, # Don't automatically raise error on non-zero exit
            encoding='utf-8' # Specify encoding
        )

        if process.returncode != 0:
            logger.error(f"Merge script failed for task {task_uuid_str}. Return code: {process.returncode}")
            logger.error(f"Script stderr:\n{process.stderr}")
            logger.error(f"Script stdout:\n{process.stdout}")
            # Try to delete potentially incomplete output file
            try:
                output_file_path = Path(output_abs_path)
                if output_file_path.exists():
                    await run_in_threadpool(os.remove, output_file_path)
                    logger.info(f"Deleted potentially incomplete output file: {output_abs_path}")
            except Exception as del_e:
                logger.error(f"Failed to delete incomplete output file {output_abs_path}: {del_e}")

            raise HTTPException(status_code=500, detail=f"Merge script failed: {process.stderr[:500]}") # Limit error detail length

        # Script succeeded, update the specific metadata field
        if target_metadata_field:
            setattr(task_meta, target_metadata_field, str(output_rel_path))
            metadata[task_uuid_str] = task_meta
            await save_metadata(metadata)
            logger.info(f"Successfully updated metadata field '{target_metadata_field}' for task {task_uuid_str}. Output: {output_rel_path}")
        else:
             # This should not happen if validation is correct
            logger.error(f"Internal logic error: target_metadata_field not set for format {request.format}")
            # Avoid saving metadata if the field name is unknown

        logger.info(f"Successfully merged VTT for task {task_uuid_str}. Output: {output_rel_path}")
        return {
            "message": f"VTT merge successful (format: {request.format}).",
            "merged_file_path": str(output_rel_path) # Return the newly generated path
        }

    except FileNotFoundError:
        logger.error(f"Merge script not found at {script_path}", exc_info=True)
        raise HTTPException(status_code=500, detail="Merge script not found.")
    except Exception as e:
        logger.error(f"Unexpected error during VTT merge for task {task_uuid_str}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error during VTT merge: {str(e)}")

# --- END: Merge VTT Endpoint ---

# --- Helper Function to Run Transcription Script and Broadcast --- 
# MODIFIED to return the relative path of the generated JSON, or None on failure
def run_transcription_script_and_notify(uuid: str, model: str) -> Optional[str]:
    """Runs transcription script, captures its output, and returns relative path on success."""
    script_path = SRC_DIR / "tasks" / "transcribe_whisperx.py"
    python_executable = sys.executable
    command = [python_executable, str(script_path), '--uuid', uuid, '--model', model]
    workspace_root = BACKEND_DIR.parent
    logger.info(f"Running WhisperX transcription command in {workspace_root}: {' '.join(command)}")

    relative_path = None # Initialize return value
    error_message = None

    try:
        # Run with check=False, capture output
        process = subprocess.run(
            command,
            capture_output=True,
            text=True,
            check=False, # Don't raise error on non-zero exit
            encoding='utf-8',
            cwd=workspace_root
        )

        if process.returncode == 0:
            logger.info(f"WhisperX script completed successfully for UUID {uuid}. Output:\n{process.stdout}")
            if process.stderr:
                 logger.warning(f"WhisperX script for UUID {uuid} produced stderr:\n{process.stderr}")
            # Try to parse the success JSON from stdout
            try:
                # Find the last line of stdout which should contain the final JSON status
                last_line = process.stdout.strip().splitlines()[-1]
                result_json = json.loads(last_line)
                if result_json.get("status") == "completed" and result_json.get("relative_path"):
                    relative_path = result_json["relative_path"]
                    logger.info(f"Successfully parsed relative path from script output: {relative_path}")
                else:
                    error_message = "Script finished but did not report completed status or relative path in output."
                    logger.error(f"{error_message} Last line: {last_line}")
            except (json.JSONDecodeError, IndexError) as e:
                error_message = f"Failed to parse JSON status from script stdout: {e}"
                logger.error(f"{error_message} stdout:\n{process.stdout}")
        else:
            # Script failed
            logger.error(f"WhisperX script failed for UUID {uuid}. Return code: {process.returncode}")
            logger.error(f"Stderr:\n{process.stderr}")
            logger.error(f"Stdout:\n{process.stdout}")
            error_message = f"Script failed with code {process.returncode}. Stderr: {process.stderr[:200]}..."

    except Exception as e:
        logger.error(f"An unexpected error occurred running WhisperX script process for UUID {uuid}: {e}", exc_info=True)
        error_message = f"Unexpected error running script: {e}"

    # --- REMOVED WebSocket Notification --- 
    # Notification will now be handled by the endpoint AFTER metadata is saved.

    if error_message:
        # Log any error encountered during the process
        logger.error(f"WhisperX helper function encountered error for {uuid}: {error_message}")

    return relative_path # Return the path or None

# --- POST Endpoint to Start WhisperX Transcription (Modified to Await) ---
@app.post("/api/tasks/{task_uuid}/transcribe_whisperx", response_model=TaskMetadata)
async def transcribe_whisperx_endpoint(
    task_uuid: UUID,
    request: TranscribeRequest # Changed from TranscribeWhisperXRequest to TranscribeRequest
):
    uuid_str = str(task_uuid)
    model = request.model
    logger.info(f"Received SYNC request to start WhisperX transcription for UUID {uuid_str} with model {model}")

    metadata = await load_metadata()
    task_meta = metadata.get(uuid_str)
    # ... (Existing checks: not found, archived, transcript exists, audio exists) ...
    if not task_meta: raise HTTPException(status_code=404, detail="Task not found")
    if task_meta.archived: raise HTTPException(status_code=400, detail="Cannot transcribe archived task.")
    # Allow re-transcription if path exists but maybe failed before?
    # if task_meta.whisperx_json_path: raise HTTPException(status_code=409, detail="WhisperX transcript already exists. Delete it first.")
    audio_path_str = task_meta.downloaded_audio_path or task_meta.extracted_wav_path
    if not audio_path_str: raise HTTPException(status_code=400, detail="Audio file path not found...")
    expected_audio_path = DATA_DIR / audio_path_str
    if not expected_audio_path.exists(): raise HTTPException(status_code=400, detail=f"Audio file not found at {expected_audio_path}...")

    # --- Run the script synchronously in threadpool and get relative path --- 
    relative_path = None
    script_error = None
    try:
        # Await the helper which now returns the relative path or None
        relative_path = await run_in_threadpool(run_transcription_script_and_notify, uuid_str, model)
        if relative_path:
            logger.info(f"WhisperX transcription script completed successfully for {uuid_str}. Relative path: {relative_path}")
        else:
            logger.error(f"WhisperX transcription script helper returned None (failure) for {uuid_str}.")
            # Capture a generic error message if path is None
            script_error = "Transcription script failed or did not return a valid path."

    except Exception as e:
        # This catches errors *running* the helper in threadpool itself
        logger.error(f"Error running transcription helper in threadpool for {uuid_str}: {e}", exc_info=True)
        script_error = f"Error executing transcription task: {e}"

    # --- Update Metadata and Broadcast --- 
    task_status = "failed"
    updated_task_data_dict = None
    try:
        # Reload metadata AFTER the script has potentially run
        current_metadata = await load_metadata()
        current_task_meta = current_metadata.get(uuid_str)

        if not current_task_meta:
            # Task disappeared somehow? Log error, cannot proceed.
            logger.error(f"Task {uuid_str} not found in metadata after script execution. Cannot update status or broadcast.")
            # Raise 500 as state is inconsistent
            raise HTTPException(status_code=500, detail="Task metadata inconsistency after processing.")

        if relative_path and not script_error:
            # --- Success Path: Update Metadata --- 
            current_task_meta.whisperx_json_path = relative_path
            current_task_meta.transcription_model = model
            current_metadata[uuid_str] = current_task_meta
            await save_metadata(current_metadata)
            logger.info(f"Successfully updated metadata for task {uuid_str} with WhisperX path and model.")
            task_status = "completed"
            updated_task_data_dict = current_task_meta.dict()
        else:
            # --- Failure Path: Metadata NOT updated --- 
            logger.warning(f"Transcription failed for task {uuid_str}. Metadata will not be updated.")
            # Use the existing task data (without transcript path) for broadcast
            updated_task_data_dict = current_task_meta.dict()

        # --- Broadcast Update --- 
        message = {
            "type": "task_update",
            "status": task_status,
            "uuid": uuid_str,
            "task_data": updated_task_data_dict
        }
        if script_error and task_status == "failed":
            message["error"] = script_error # Add error detail if script failed

        await manager.broadcast(message)
        logger.info(f"Broadcasted task update for UUID {uuid_str} (Status: {task_status})")

        # --- Return Response --- 
        if task_status == "completed":
            return current_task_meta # Return the updated TaskMetadata on success
        else:
            # If script failed, raise an HTTP exception
            raise HTTPException(status_code=500, detail=script_error or "Transcription failed for unknown reasons.")

    except Exception as e:
        # Catch errors during metadata reload, save, or broadcast
        logger.error(f"Error during final metadata update or broadcast for {uuid_str}: {e}", exc_info=True)
        # Try to broadcast a generic failure if possible?
        try:
            await manager.broadcast({
                "type": "task_update",
                "status": "failed",
                "uuid": uuid_str,
                "error": f"Internal server error during final update: {e}"
            })
        except Exception as broadcast_err:
            logger.error(f"Failed to broadcast final error state for {uuid_str}: {broadcast_err}")
        # Raise 500
        raise HTTPException(status_code=500, detail="Internal server error during task finalization.")

# --- DELETE Endpoint to Remove WhisperX Transcript ---
@app.delete("/api/tasks/{task_uuid}/transcribe_whisperx", status_code=200)
async def delete_whisperx_transcript(task_uuid: UUID):
    uuid_str = str(task_uuid)
    logger.info(f"Received request to delete WhisperX transcript for UUID {uuid_str}")

    metadata = await load_metadata()
    task_meta = metadata.get(uuid_str)

    if not task_meta:
        logger.warning(f"Task {uuid_str} not found for WhisperX transcript deletion.")
        raise HTTPException(status_code=404, detail="Task not found")

    if not task_meta.whisperx_json_path:
        logger.info(f"No WhisperX transcript path found for task {uuid_str}. Nothing to delete.")
        task_meta.whisperx_json_path = None
        task_meta.transcription_model = None
        await save_metadata(metadata)
        return {"message": "No WhisperX transcript found, metadata cleared.", "task_data": task_meta.dict()}


    transcript_rel_path = task_meta.whisperx_json_path
    # Construct absolute path relative to DATA_DIR
    transcript_abs_path = DATA_DIR / transcript_rel_path
    logger.info(f"Attempting to delete transcript file at: {transcript_abs_path}")

    deleted = False
    if transcript_abs_path.exists() and transcript_abs_path.is_file():
        try:
            await run_in_threadpool(os.remove, transcript_abs_path)
            logger.info(f"Successfully deleted transcript file: {transcript_abs_path}")
            deleted = True
        except OSError as e:
            logger.error(f"Failed to delete transcript file {transcript_abs_path}: {e}")
        except Exception as e:
            logger.error(f"Unexpected error deleting file {transcript_abs_path}: {e}", exc_info=True)

    # Update metadata regardless of file deletion success
    task_meta.whisperx_json_path = None
    task_meta.transcription_model = None

    await save_metadata(metadata)
    logger.info(f"Updated metadata for task {uuid_str}, removing WhisperX transcript info.")

    # Return the updated task metadata as expected by the frontend
    return {"message": f"WhisperX transcript data {'deleted and ' if deleted else ''}metadata cleared for task {uuid_str}.", "task_data": task_meta.dict()}

# --- WebSocket Endpoint ---
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            # Keep connection alive, maybe listen for client messages if needed
            data = await websocket.receive_text() 
            # Example: Respond to a ping or specific client request
            # await manager.send_personal_message(f"Message text was: {data}", websocket)
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket error for {websocket.client}: {e}", exc_info=True)
        if websocket in manager.active_connections: # Ensure disconnect even on unexpected errors
             manager.disconnect(websocket)

# --- NEW: Endpoint to Create Video from Audio+Image ---
class CreateVideoResponse(BaseModel):
    message: str
    output_path: str

@app.post("/api/tasks/{task_uuid}/create_video", response_model=CreateVideoResponse)
async def create_video_endpoint(task_uuid: UUID):
    task_uuid_str = str(task_uuid)
    logger.info(f"Received request to create video for task: {task_uuid_str}")
    metadata = await load_metadata()

    if task_uuid_str not in metadata:
        logger.error(f"Task not found: {task_uuid_str}")
        raise HTTPException(status_code=404, detail="Task not found")

    task_meta = metadata[task_uuid_str]
    task_dir = BASE_DIR / task_uuid_str

    # --- Input Validation ---
    if task_meta.platform not in ["xiaoyuzhou", "podcast"]: # Assuming 'podcast' is a valid platform identifier
        logger.warning(f"Platform '{task_meta.platform}' not supported for video creation for task {task_uuid_str}")
        raise HTTPException(status_code=400, detail=f"Platform '{task_meta.platform}' not supported for this operation.")

    if not task_meta.thumbnail_path:
        logger.warning(f"Thumbnail path is missing for task {task_uuid_str}")
        raise HTTPException(status_code=400, detail="Thumbnail path is missing.")
        
    if not task_meta.downloaded_audio_path:
        logger.warning(f"Downloaded audio path is missing for task {task_uuid_str}")
        raise HTTPException(status_code=400, detail="Downloaded audio path is missing.")

    thumbnail_full_path = BASE_DIR / task_meta.thumbnail_path
    audio_full_path = BASE_DIR / task_meta.downloaded_audio_path
    output_video_rel_path = Path(task_uuid_str) / "video_best.mp4" # Relative path for metadata
    output_video_full_path = BASE_DIR / output_video_rel_path # Full path for script

    if not thumbnail_full_path.exists():
        logger.error(f"Thumbnail file not found at expected location: {thumbnail_full_path}")
        raise HTTPException(status_code=404, detail=f"Thumbnail file not found: {task_meta.thumbnail_path}")
        
    if not audio_full_path.exists():
        logger.error(f"Audio file not found at expected location: {audio_full_path}")
        raise HTTPException(status_code=404, detail=f"Audio file not found: {task_meta.downloaded_audio_path}")

    # --- Run Video Creation Script (Synchronously for now, consider BackgroundTasks for long processes) ---
    try:
        logger.info(f"Starting video creation for {task_uuid_str}: Image='{thumbnail_full_path}', Audio='{audio_full_path}', Output='{output_video_full_path}'")
        
        # Use run_in_threadpool to avoid blocking the event loop
        await run_in_threadpool(
            create_video_from_audio_image, 
            image_path=str(thumbnail_full_path), 
            audio_path=str(audio_full_path), 
            output_path=str(output_video_full_path)
        )

        # --- Verify Output and Update Metadata ---
        if not output_video_full_path.exists():
             logger.error(f"Video creation script ran but output file not found: {output_video_full_path}")
             raise HTTPException(status_code=500, detail="Video creation failed: Output file not generated.")

        logger.info(f"Video created successfully: {output_video_full_path}")
        # Update metadata
        if task_meta.media_files is None: # Ensure media_files exists
            task_meta.media_files = {}
        task_meta.media_files["best"] = str(output_video_rel_path) # Store relative path

        metadata[task_uuid_str] = task_meta
        await save_metadata(metadata)
        
        # Broadcast update
        await manager.broadcast({"type": "metadata_update", "payload": {task_uuid_str: task_meta.dict()}})
        
        logger.info(f"Metadata updated for task {task_uuid_str} with new video file: {output_video_rel_path}")
        
        return CreateVideoResponse(
            message="Video created successfully", 
            output_path=str(output_video_rel_path)
        )

    except Exception as e: # Catch errors from the script execution or file checks
        logger.error(f"Error during video creation for task {task_uuid_str}: {e}", exc_info=True)
        # Attempt to clean up potentially incomplete output file
        if output_video_full_path.exists():
            try:
                os.remove(output_video_full_path)
                logger.info(f"Cleaned up partial output file: {output_video_full_path}")
            except OSError as rm_err:
                logger.error(f"Failed to clean up partial output file {output_video_full_path}: {rm_err}")
        raise HTTPException(status_code=500, detail=f"Video creation failed: {str(e)}")

# --- Endpoint to Open Task Folder --- 
@app.post("/api/tasks/{task_uuid}/open_folder", status_code=200)
async def open_task_folder_endpoint(task_uuid: UUID):
    """
    Opens the data folder for the specified task in the system's file explorer.
    """
    task_uuid_str = str(task_uuid)
    logger.info(f"Received request to open folder for task: {task_uuid_str}")
    
    # No need to load metadata unless we need to validate existence first
    # metadata = await load_metadata()
    # if task_uuid_str not in metadata:
    #     logger.warning(f"Task not found for opening folder: {task_uuid_str}")
    #     raise HTTPException(status_code=404, detail="Task not found")

    task_data_dir = DATA_DIR / task_uuid_str

    if not task_data_dir.exists() or not task_data_dir.is_dir():
        logger.warning(f"Task data directory not found or is not a directory: {task_data_dir}")
        raise HTTPException(status_code=404, detail=f"Task data directory not found: {task_data_dir}")

    try:
        logger.info(f"Attempting to open folder: {task_data_dir}")
        cmd = []
        if sys.platform == "win32":
            # Use os.startfile on Windows for better behavior?
            # os.startfile(task_data_dir) 
            # Or stick with explorer:
            cmd = ["explorer", str(task_data_dir)]
        elif sys.platform == "darwin": # macOS
            cmd = ["open", str(task_data_dir)]
        else: # Linux and other Unix-like
            cmd = ["xdg-open", str(task_data_dir)]
        
        # Run the command asynchronously in a threadpool to avoid blocking
        process = await run_in_threadpool(
            subprocess.run,
            cmd,
            check=True, # Raise exception on non-zero exit code
            capture_output=True # Capture output/errors if needed
        )
        logger.info(f"Successfully executed command to open folder: {' '.join(cmd)}")
        return {"message": f"Request to open folder {task_data_dir} successful."}

    except FileNotFoundError:
        # This might happen if xdg-open/open/explorer is not in PATH
        logger.error(f"Command to open file explorer not found (platform: {sys.platform}). Command: {' '.join(cmd)}")
        raise HTTPException(status_code=501, detail="File explorer command not found on the server.")
    except subprocess.CalledProcessError as e:
        logger.error(f"Error executing command to open folder {task_data_dir}: {e}")
        logger.error(f"Command stdout: {e.stdout}")
        logger.error(f"Command stderr: {e.stderr}")
        raise HTTPException(status_code=500, detail=f"Failed to open folder on server: {e.stderr or e.stdout}")
    except Exception as e:
        logger.error(f"Unexpected error opening folder {task_data_dir}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred on the server while trying to open the folder.")

# --- END NEW Endpoint --- 

# --- END: New GET Markdown Endpoint ---

# --- START: New List Markdown Files Endpoint ---
class MarkdownFilesResponse(BaseModel):
    files: List[str]

# Restore response_model and original logic
@app.get("/api/tasks/{task_uuid}/markdown/list", response_model=MarkdownFilesResponse) # Ensure path is /list
async def list_markdown_files(task_uuid: UUID): # Use UUID type hint
    """
    Lists all markdown (.md) files within the specified task's directory.
    """
    task_uuid_str = str(task_uuid) 
    # Minimal logging

    # Construct the path to the task directory (removed markdown subdirectory)
    task_dir = DATA_DIR / task_uuid_str

    if not task_dir.exists():
        logger.warning(f"Task directory does not exist: {task_dir}")
        return MarkdownFilesResponse(files=[]) 
    if not task_dir.is_dir():
        logger.warning(f"Path exists but is not a directory: {task_dir}") 
        return MarkdownFilesResponse(files=[]) 
        
    # Confirmed exists and is directory

    try:
        markdown_files = []
        # Scan for markdown files in the task directory
        for item in task_dir.iterdir():
            is_file = item.is_file()
            if is_file:
                is_md = item.name.lower().endswith('.md')
                if is_md:
                    markdown_files.append(item.name)
        
        # Also check for a markdown subdirectory if it exists
        markdown_subdir = task_dir / "markdown"
        if markdown_subdir.exists() and markdown_subdir.is_dir():
            for item in markdown_subdir.iterdir():
                is_file = item.is_file()
                if is_file:
                    is_md = item.name.lower().endswith('.md')
                    if is_md:
                        # Prepend with subdirectory for correct path reference
                        markdown_files.append(f"markdown/{item.name}")
        
        logger.info(f"Found {len(markdown_files)} markdown files for task {task_uuid_str}") 
        # Return the sorted list inside the response model
        response_data = MarkdownFilesResponse(files=sorted(markdown_files))
        return response_data

    except Exception as e:
        logger.error(f"Error listing markdown files for task {task_uuid_str}: {e}", exc_info=True)
        # Raise 500 for internal errors.
        raise HTTPException(status_code=500, detail="Error listing markdown files")
# --- END: New List Markdown Files Endpoint ---


# --- START: Existing GET File Endpoint (for reference) ---
# This endpoint is used by the frontend to fetch the content of a specific file
@app.api_route("/api/tasks/{task_uuid}/files/{filename}", methods=["GET", "HEAD"], response_class=FileResponse)
async def get_task_file(task_uuid: UUID, filename: str):
    # ... existing implementation ...
    pass # Keep existing code
# --- END: Existing GET File Endpoint ---



# --- START: New List Files Endpoint ---
@app.get("/api/tasks/{task_uuid}/files/list", response_model=List[str])
async def list_task_files(
    task_uuid: UUID,
    extension: Optional[str] = Query(None, description="Filter by file extension (e.g., .txt, .md)")
):
    """
    Lists files within the specified task's data directory, optionally filtering by extension.
    Only lists files, not directories.
    """
    task_uuid_str = str(task_uuid)
    logger.info(f"Request to list files for task {task_uuid_str} (extension filter: {extension})")

    task_data_dir = DATA_DIR / task_uuid_str

    if not task_data_dir.exists() or not task_data_dir.is_dir():
        logger.warning(f"Task data directory not found: {task_data_dir}")
        raise HTTPException(status_code=404, detail="Task data directory not found")

    try:
        all_files = []
        for item in task_data_dir.iterdir():
            if item.is_file():
                # Apply extension filter if provided
                if extension:
                    # Ensure extension starts with a dot for consistent comparison
                    filter_ext = extension if extension.startswith('.') else f".{extension}"
                    if item.name.lower().endswith(filter_ext.lower()):
                        all_files.append(item.name)
                else:
                    # No filter, add all files
                    all_files.append(item.name)
        
        logger.info(f"Found {len(all_files)} files for task {task_uuid_str} matching filter '{extension}'")
        return sorted(all_files) # Return sorted list

    except Exception as e:
        logger.error(f"Error listing files in directory {task_data_dir}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Error listing files")
# --- END: New List Files Endpoint ---

# --- Pydantic Models for Cut API --- 

class Segment(BaseModel):
    start: float = Field(..., description="Start time in seconds")
    end: float = Field(..., description="End time in seconds")

class CutRequest(BaseModel):
    media_identifier: str = Field(..., description="Identifier for the media file (e.g., relative path)")
    segments: List[Segment] = Field(..., description="List of time segments to keep")
    embed_subtitle_lang: Optional[Literal['en', 'zh-Hans', 'bilingual', 'srt', 'none']] = Field(
        default='none', 
        description="Language of subtitles to embed/burn into the video. 'none' for no subtitles."
    )
    subtitle_type: Optional[Literal['vtt', 'srt']] = Field(
        default='vtt',
        description="Type of subtitle file being used. 'vtt' for WebVTT, 'srt' for SubRip."
    )
    output_format: Literal['video', 'wav'] = Field(
        default='video',
        description="Output format: 'video' for MP4, 'wav' for audio-only WAV"
    )

class CutResponse(BaseModel):
    job_id: str
    status: str
    message: str

class CutStatusResponse(BaseModel):
    job_id: str
    status: str # e.g., "processing", "completed", "failed"
    message: Optional[str] = None
    output_path: Optional[str] = None # Relative path to the output file if completed
    output_files: Optional[Dict[str, Any]] = None # Contains paths to all generated files (video, subtitle_vtt, subtitle_ass, etc.)

# --- In-memory storage for job statuses (Replace with Redis/DB in production) --- 
cut_job_statuses: Dict[str, Dict[str, Any]] = {}

# --- Background Task Function for FFMPEG --- 

# Helper function for escaping paths for ffmpeg filter
def escape_ffmpeg_filter_path(path_str: str) -> str:
    # Escape characters problematic in ffmpeg filters: \, ', :, [, ], ;, ,, =, space
    escaped = path_str.replace('\\', '\\\\')
    escaped = escaped.replace("'", "\\'")
    escaped = escaped.replace(":", "\\:")
    escaped = escaped.replace("[", "\\[")
    escaped = escaped.replace("]", "\\]")
    escaped = escaped.replace(";", "\\;")
    escaped = escaped.replace(",", "\\,")
    escaped = escaped.replace("=", "\\=")
    escaped = escaped.replace(" ", "\\ ")
    return escaped

# 将VTT转换为ASS格式的函数 - 更可靠的字幕烧录格式
def convert_vtt_to_ass(vtt_file_path, output_ass_path, segments: List[Dict], is_bilingual=False):
    try:
        import webvtt
        from pathlib import Path
        import re
        
        # 确保输出目录存在
        Path(output_ass_path).parent.mkdir(parents=True, exist_ok=True)
        
        # 读取VTT文件
        vtt_obj = webvtt.read(str(vtt_file_path))
        logger.info(f"转换VTT到ASS: 读取到{len(vtt_obj.captions)}个字幕条目...")
        
        # 将segments时间范围重组为更方便处理的格式
        segment_duration_map = []  # [原片段起始, 原片段结束, 输出片段起始, 输出片段延续时长]
        output_time_position = 0.0
        
        for seg in segments:
            if seg['end'] <= seg['start']:
                continue  # 跳过无效片段
            
            segment_duration = seg['end'] - seg['start']
            segment_duration_map.append([
                seg['start'],         # 原片段起始时间
                seg['end'],           # 原片段结束时间
                output_time_position, # 该片段在输出视频中的起始位置
                segment_duration      # 该片段持续时间
            ])
            output_time_position += segment_duration
        
        # 将时间戳从秒转换为ASS格式 (h:mm:ss.cc)
        def format_time(seconds_val):
            h = int(seconds_val / 3600)
            m = int((seconds_val % 3600) / 60)
            s = int(seconds_val % 60)
            cs = int((seconds_val - int(seconds_val)) * 100)  # ASS使用厘秒(centiseconds)
            return f"{h}:{m:02d}:{s:02d}.{cs:02d}"
        
        # ASS文件头 - 使用专业字幕样式，统一对齐方式解决位置不稳定问题
        ass_header = '''[Script Info]
ScriptType: v4.00+
PlayResX: 1280
PlayResY: 720
ScaledBorderAndShadow: yes
YCbCr Matrix: TV.709

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
; 中文样式 - 黄色字体，底部对齐(2)以保持位置稳定
Style: Chinese,PingFang SC,42,&H0000FFFF,&H000000FF,&H00000000,&H50000000,1,0,0,0,100,100,0,0,3,2.5,0,2,20,20,30,1
; 英文样式 - 底部显示
Style: English,Arial,37,&H00FFFFFF,&H000000FF,&H00000000,&H50000000,0,0,0,0,100,100,0,0,3,2.0,0,2,20,20,30,1
; 单独中文样式 - 黄色字体
Style: Chinese-Only,PingFang SC,42,&H0000FFFF,&H000000FF,&H00000000,&H50000000,1,0,0,0,100,100,0,0,3,2.5,0,2,20,20,30,1
; 单独英文样式
Style: English-Only,Arial,38,&H00FFFFFF,&H000000FF,&H00000000,&H50000000,0,0,0,0,100,100,0,0,3,2.2,0,2,20,20,30,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
'''
        
        # 写入ASS文件
        with open(output_ass_path, 'w', encoding='utf-8') as f:
            f.write(ass_header)
            
            processed_count = 0
            skipped_count = 0
            
            # 不再使用简单去重，而是考虑字幕的显示时间和间隔
            # 改为使用最近显示的字幕记录和它们的结束时间
            recent_subtitles = {}  # 格式: {文本内容: 结束时间}
            
            # 最小间隔时间（秒），只有当两个相同内容的字幕间隔超过这个值，才会再次显示
            MIN_REPEAT_INTERVAL = 2.0
            
            # 保留原始字幕时间，只进行必要的段落映射
            assigned_subtitles = []  # 先收集所有字幕，然后再按顺序处理
            
            # 第一遍：映射所有字幕并确定它们在新时间线上的位置
            for i, caption in enumerate(vtt_obj.captions):
                # 原始字幕时间戳
                caption_start_s = caption.start_in_seconds
                caption_end_s = caption.end_in_seconds
                caption_text = caption.text.strip()
                
                if not caption_text:
                    skipped_count += 1
                    continue
                
                # 查找此字幕在哪个片段中
                output_start = None
                output_end = None
                matching_segment = None
                
                for seg_info in segment_duration_map:
                    original_start, original_end, output_start_time, duration = seg_info
                    
                    # 字幕完全在片段内
                    if original_start <= caption_start_s and caption_end_s <= original_end:
                        # 直接保留原始持续时间，只映射位置
                        time_offset_in_segment = caption_start_s - original_start
                        output_start = output_start_time + time_offset_in_segment
                        output_end = output_start + (caption_end_s - caption_start_s)
                        
                        # 确保字幕不超出段落边界
                        if output_end > output_start_time + duration:
                            output_end = output_start_time + duration
                        
                        matching_segment = seg_info
                        break
                    
                    # 字幕部分在片段内（开始点在片段内）
                    elif original_start <= caption_start_s < original_end:
                        # 保留原始时间点但截断到段落结束
                        time_offset_in_segment = caption_start_s - original_start
                        output_start = output_start_time + time_offset_in_segment
                        output_end = output_start_time + duration  # 截断到段落结束
                        
                        matching_segment = seg_info
                        break
                    
                    # 字幕部分在片段内（结束点在片段内）
                    elif original_start < caption_end_s <= original_end:
                        # 从段落开始但保留原始持续时间
                        output_start = output_start_time  # 从段落开始
                        output_end = output_start_time + (caption_end_s - original_start)
                        
                        matching_segment = seg_info
                        break
                    
                    # 字幕完全覆盖了片段
                    elif caption_start_s <= original_start and caption_end_s >= original_end:
                        # 映射到整个段落
                        output_start = output_start_time
                        output_end = output_start_time + duration
                        
                        matching_segment = seg_info
                        break
                
                # 如果没找到匹配的片段，或者计算出的时间无效
                if output_start is None or output_end is None or output_start >= output_end:
                    skipped_count += 1
                    if i < 5:
                        logger.debug(f"跳过字幕 #{i}: 未找到匹配片段 '{caption_text[:20]}...' @ {caption_start_s:.2f}-{caption_end_s:.2f}")
                    continue
                
                # 确保字幕持续时间合理 - 最小持续1秒，保持原始时长
                duration = output_end - output_start
                if duration < 1.0:
                    output_end = output_start + max(1.0, caption_end_s - caption_start_s)
                
                # 格式化为ASS时间格式
                start_str = format_time(output_start)
                end_str = format_time(output_end)
                
                # 检测中英文
                zh_part = None
                en_part = None
                
                # 首先检查是否是双语（有'\n'或'\r\n'分隔）
                if is_bilingual or '\n' in caption_text:
                    lines = caption_text.split('\n')
                    # 只考虑前两行，即使有更多行
                    if len(lines) >= 2:
                        line1 = lines[0].strip()
                        line2 = lines[1].strip()
                        
                        # 简单判断哪一行是英文，哪一行是中文 - 基于规则：
                        # 1. 含有中文字符的为中文
                        # 2. 不含中文字符的为英文
                        has_zh_line1 = bool(re.search(r'[\u4e00-\u9fff]', line1))
                        has_zh_line2 = bool(re.search(r'[\u4e00-\u9fff]', line2))
                        
                        # 双语模式下智能分配
                        if has_zh_line1 and has_zh_line2:
                            # 两行都有中文，取较短的作为中文（可能是总结）
                            if len(line1) <= len(line2):
                                zh_part = line1
                                en_part = line2  # 这可能也是中文
                            else:
                                zh_part = line2
                                en_part = line1  # 这可能也是中文
                        elif has_zh_line1:
                            zh_part = line1
                            en_part = line2
                        elif has_zh_line2:
                            zh_part = line2
                            en_part = line1
                        else:
                            # 没有中文，按英文处理全部
                            en_part = caption_text  
                    else:
                        # 只有一行
                        has_zh = bool(re.search(r'[\u4e00-\u9fff]', caption_text))
                        if has_zh:
                            zh_part = caption_text
                        else:
                            en_part = caption_text
                else:
                    # 单行文本
                    has_zh = bool(re.search(r'[\u4e00-\u9fff]', caption_text))
                    if has_zh:
                        zh_part = caption_text
                    else:
                        en_part = caption_text
                
                # 添加到待处理列表
                assigned_subtitles.append({
                    'index': i,
                    'start': output_start,
                    'end': output_end,
                    'start_str': start_str,
                    'end_str': end_str,
                    'zh_part': zh_part,
                    'en_part': en_part,
                    'full_text': caption_text,
                    'is_bilingual': bool(zh_part and en_part),
                    'duration': output_end - output_start
                })
            
            # 第二遍：按时间排序处理字幕，智能去重
            assigned_subtitles.sort(key=lambda x: x['start'])
            
            for i, sub in enumerate(assigned_subtitles):
                start_str = sub['start_str']
                end_str = sub['end_str']
                start_time = sub['start']
                
                # 检查中文部分
                zh_part = sub['zh_part']
                en_part = sub['en_part']
                
                # 查看是否有有效的中文或英文内容
                has_zh = bool(zh_part)
                has_en = bool(en_part)
                
                # 确定是否显示这个字幕（基于时间间隔）
                show_zh = False
                show_en = False
                
                # 检查中文部分是否应该显示
                if has_zh:
                    last_end_time = recent_subtitles.get(zh_part, 0)
                    # 如果这是首次出现，或者距离上次显示已经超过间隔时间
                    if last_end_time == 0 or (start_time - last_end_time) >= MIN_REPEAT_INTERVAL:
                        show_zh = True
                        recent_subtitles[zh_part] = sub['end']  # 更新结束时间
                
                # 检查英文部分是否应该显示
                if has_en:
                    last_end_time = recent_subtitles.get(en_part, 0)
                    if last_end_time == 0 or (start_time - last_end_time) >= MIN_REPEAT_INTERVAL:
                        show_en = True
                        recent_subtitles[en_part] = sub['end']  # 更新结束时间
                
                # 双语模式且都显示时，垂直排列（中文在上，英文在下）
                if is_bilingual and has_zh and has_en and show_zh and show_en:
                    # 处理中文，添加特殊垂直偏移实现双语垂直排列
                    processed_zh = zh_part.replace('\n', '\\N').replace('\\n', '\\N')
                    processed_en = en_part.replace('\n', '\\N').replace('\\n', '\\N')
                    
                    # 使用特殊标记强制位置：
                    # 注意：维持垂直偏移，但使用底部对齐(2)以保持位置统一
                    f.write(f"Dialogue: 0,{start_str},{end_str},Chinese,,0,0,0,,{{\\pos(640,650)\\an2}}{processed_zh}\\N{processed_en}\n")
                    processed_count += 2  # 算作两行
                    continue

                # 写出中文部分（如果有且应该显示）
                if has_zh and show_zh:
                    processed_text = zh_part.replace('\n', '\\N').replace('\\n', '\\N')
                    f.write(f"Dialogue: 0,{start_str},{end_str},Chinese-Only,,0,0,0,,{processed_text}\n")
                    processed_count += 1
                
                # 写出英文部分（如果有且应该显示）
                if has_en and show_en:
                    processed_text = en_part.replace('\n', '\\N').replace('\\n', '\\N')
                    f.write(f"Dialogue: 0,{start_str},{end_str},English-Only,,0,0,0,,{processed_text}\n")
                    processed_count += 1
                
                # 处理完整文本（如果没有独立的中英文部分）
                if not has_zh and not has_en and sub['full_text']:
                    full_text = sub['full_text']
                    last_end_time = recent_subtitles.get(full_text, 0)
                    
                    if last_end_time == 0 or (start_time - last_end_time) >= MIN_REPEAT_INTERVAL:
                        # 根据内容判断样式
                        style = "Chinese-Only" if bool(re.search(r'[\u4e00-\u9fff]', full_text)) else "English-Only"
                        processed_text = full_text.replace('\n', '\\N').replace('\\n', '\\N')
                        
                        f.write(f"Dialogue: 0,{start_str},{end_str},{style},,0,0,0,,{processed_text}\n")
                        recent_subtitles[full_text] = sub['end']
                        processed_count += 1
            
            logger.info(f"VTT到ASS转换完成：处理了{processed_count}条ASS对话行，跳过了{skipped_count}条，保留原始时长且智能去除重复")
            
            # 确保至少有一条字幕
            if processed_count == 0:
                f.write(f"Dialogue: 0,0:00:00.00,0:00:05.00,English-Only,,0,0,0,,No subtitles processed\n")
                
            return True
    except Exception as e:
        logger.error(f"Convert VTT to ASS Error: {e}", exc_info=True)
        return False

def run_ffmpeg_cut(
    task_uuid_str: str, 
    input_path: Path, 
    output_path: Path, 
    segments: List[Dict], 
    job_id: str, 
    embed_subtitle_lang: Optional[str],
    vtt_files: Dict[str, Optional[str]], # Pass the task's vtt_files dict
    output_format: str = 'video',
    subtitle_type: str = 'vtt'  # Add subtitle type parameter
):
    """Runs the ffmpeg cut command in the background, potentially embedding subtitles."""
    global cut_job_statuses
    logger.info(f"[Job {job_id}] Starting ffmpeg cut for {task_uuid_str} (Embed: {embed_subtitle_lang}, Format: {output_format})")
    cut_job_statuses[job_id]["status"] = "processing"

    # 临时文件列表，用于后续清理
    temp_files_to_clean = []
    # 输出文件列表，用于跟踪生成的文件
    output_files = {}

    # 根据输出格式修改输出路径
    if output_format == 'wav':
        output_path = output_path.with_suffix('.wav')
        output_files["audio"] = str(output_path.relative_to(DATA_DIR))
    else:
        output_files["video"] = str(output_path.relative_to(DATA_DIR))

    # 确保在任何情况下都清理临时文件的函数
    def cleanup_temp_files():
        if not temp_files_to_clean:
            return
            
        logger.info(f"[Job {job_id}] Cleaning up {len(temp_files_to_clean)} temporary files")
        for temp_file in temp_files_to_clean:
            if temp_file and temp_file.exists():
                try:
                    temp_file.unlink()
                    logger.debug(f"[Job {job_id}] Removed temporary file: {temp_file}")
                except Exception as e:
                    logger.error(f"[Job {job_id}] Failed to remove temporary file {temp_file}: {e}")

    try:
        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        if not segments:
            raise ValueError("No segments provided for cutting.")

        # Ensure output directory exists
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # --- Subtitle Setup --- 
        subtitle_filter = None
        subtitle_vtt_path = None
        subtitle_srt_path = None
        subtitle_ass_path = None
        
        if embed_subtitle_lang and embed_subtitle_lang != 'none':
            logger.info(f"[Job {job_id}] Setting up subtitles: {embed_subtitle_lang}, type: {subtitle_type}")
            
            source_subtitle_path_for_conversion = None
            
            if subtitle_type == 'srt' and embed_subtitle_lang == 'srt':
                # Handle SRT files
                logger.info(f"[Job {job_id}] Processing SRT subtitle")
                
                # Look for SRT files in the task directory
                task_dir = DATA_DIR / task_uuid_str
                srt_files = list(task_dir.glob("*.srt"))
                
                if srt_files:
                    source_srt_path = srt_files[0]  # Use first SRT file found
                    logger.info(f"[Job {job_id}] Found SRT file: {source_srt_path}")
                    
                    # Create adjusted SRT file
                    temp_dir = output_path.parent / ".temp"
                    temp_dir.mkdir(exist_ok=True)
                    
                    output_srt_filename = f"{output_path.stem}.srt"
                    output_srt_path = temp_dir / output_srt_filename
                    
                    if create_adjusted_srt_file(source_srt_path, output_srt_path, segments):
                        subtitle_srt_path = str(output_srt_path.relative_to(DATA_DIR))
                        output_files["subtitle_srt"] = subtitle_srt_path
                        temp_files_to_clean.append(output_srt_path)
                        logger.info(f"[Job {job_id}] Created adjusted SRT file: {output_srt_path}")
                        
                        # For video embedding, we still need to convert SRT to ASS for ffmpeg
                        source_subtitle_path_for_conversion = source_srt_path
                    else:
                        logger.error(f"[Job {job_id}] Failed to create adjusted SRT file")
                else:
                    logger.warning(f"[Job {job_id}] No SRT files found in task directory")
            else:
                # Handle VTT files (existing logic)
                en_vtt_rel = vtt_files.get('en')
                zh_vtt_rel = vtt_files.get('zh-Hans')
                
                en_vtt_abs = DATA_DIR / en_vtt_rel if en_vtt_rel and (DATA_DIR / en_vtt_rel).exists() else None
                zh_vtt_abs = DATA_DIR / zh_vtt_rel if zh_vtt_rel and (DATA_DIR / zh_vtt_rel).exists() else None
                
                source_vtt_path_for_conversion = None

                if embed_subtitle_lang == 'en' and en_vtt_abs:
                    source_vtt_path_for_conversion = en_vtt_abs
                    source_subtitle_path_for_conversion = en_vtt_abs
                    logger.info(f"[Job {job_id}] Selected English VTT for conversion: {source_vtt_path_for_conversion}")
                elif embed_subtitle_lang == 'zh-Hans' and zh_vtt_abs:
                    source_vtt_path_for_conversion = zh_vtt_abs
                    source_subtitle_path_for_conversion = zh_vtt_abs
                    logger.info(f"[Job {job_id}] Selected Chinese VTT for conversion: {source_vtt_path_for_conversion}")
                elif embed_subtitle_lang == 'bilingual' and en_vtt_abs and zh_vtt_abs:
                    logger.info(f"[Job {job_id}] Found both VTTs for bilingual embedding. Generating combined VTT first.")
                    try:
                        from webvtt import WebVTT # Keep import inside try for this specific feature
                        
                        vtt_en = WebVTT().read(str(en_vtt_abs))
                        vtt_zh = WebVTT().read(str(zh_vtt_abs))

                        def format_bilingual_vtt_time(seconds_val):
                            hours = int(seconds_val // 3600)
                            minutes = int((seconds_val % 3600) // 60)
                            secs_val = int(seconds_val % 60)
                            millis = int((seconds_val - int(seconds_val)) * 1000)
                            return f"{hours:02}:{minutes:02}:{secs_val:02}.{millis:03}"

                        max_len = max(len(vtt_en.captions), len(vtt_zh.captions))
                        combined_vtt_content = "WEBVTT\n\n"
                        
                        # 添加调试日志，看看有多少字幕项
                        logger.info(f"[Job {job_id}] 读取到英文字幕 {len(vtt_en.captions)} 条，中文字幕 {len(vtt_zh.captions)} 条")
                        
                        included_caption_count = 0
                        for i in range(max_len):
                            caption_en = vtt_en.captions[i] if i < len(vtt_en.captions) else None
                            caption_zh = vtt_zh.captions[i] if i < len(vtt_zh.captions) else None
                            
                            current_start_s = None
                            current_end_s = None
                            
                            # 优先使用英文字幕的时间戳（修改逻辑：英文优先）
                            if caption_en and caption_en.start_in_seconds is not None and caption_en.end_in_seconds is not None:
                                current_start_s = caption_en.start_in_seconds
                                current_end_s = caption_en.end_in_seconds
                            elif caption_zh and caption_zh.start_in_seconds is not None and caption_zh.end_in_seconds is not None:
                                current_start_s = caption_zh.start_in_seconds
                                current_end_s = caption_zh.end_in_seconds
                            
                            if current_start_s is None or current_end_s is None:
                                logger.debug(f"[Job {job_id}] Index {i}: 跳过合并字幕，缺少时间数据")
                                continue

                            # 尝试保留原始文本格式
                            text_en = caption_en.text if caption_en and caption_en.text else ""
                            text_zh = caption_zh.text if caption_zh and caption_zh.text else ""
                            
                            combined_text = ""
                            if text_zh and text_en:
                                # 确保英文放在前面（上面），中文放在后面（下面）
                                combined_text = f"{text_en}\n{text_zh}"
                            elif text_zh:
                                combined_text = text_zh
                            elif text_en:
                                combined_text = text_en
                            
                            # 移除时间有效性检查，包含所有字幕
                            if combined_text:
                                start_formatted = format_bilingual_vtt_time(current_start_s)
                                end_formatted = format_bilingual_vtt_time(current_end_s)
                                # 确保正确的换行符格式
                                combined_vtt_content += f"{start_formatted} --> {end_formatted}\n{combined_text}\n\n"
                                included_caption_count += 1
                                if i < 3 or i == max_len - 1:  # 记录前三个和最后一个
                                    logger.debug(f"[Job {job_id}] 包含字幕 #{i}: {start_formatted} --> {end_formatted}, 文本: {combined_text[:30]}...")
                            else:
                                logger.debug(f"[Job {job_id}] Index {i}: 跳过 - 没有文本内容")
                        
                        logger.info(f"[Job {job_id}] 合并生成了 {included_caption_count} 条双语字幕")
                        
                        # 使用任务UUID+jobId创建一个唯一的临时文件路径，存储在隐藏目录中
                        temp_dir = output_path.parent / ".temp"
                        temp_dir.mkdir(exist_ok=True)
                        temp_bilingual_vtt_path = temp_dir / f"bilingual_{job_id}.vtt"
                        
                        with open(temp_bilingual_vtt_path, 'w', encoding='utf-8') as f:
                            f.write(combined_vtt_content)
                        source_vtt_path_for_conversion = temp_bilingual_vtt_path
                        source_subtitle_path_for_conversion = temp_bilingual_vtt_path
                        temp_files_to_clean.append(temp_bilingual_vtt_path)
                        logger.info(f"[Job {job_id}] Generated temporary bilingual VTT for conversion: {source_vtt_path_for_conversion}")

                    except ImportError:
                        logger.warning(f"[Job {job_id}] 'webvtt-py' library not installed. Cannot generate combined bilingual VTT. Skipping embed.")
                        source_vtt_path_for_conversion = None 
                        source_subtitle_path_for_conversion = None
                    except Exception as e:
                        logger.error(f"[Job {job_id}] Error generating temporary bilingual VTT: {e}", exc_info=True)
                        source_vtt_path_for_conversion = None 
                        source_subtitle_path_for_conversion = None

                # --- 准备调整后的VTT文件，用于与视频一起提供 ---
                if source_vtt_path_for_conversion and source_vtt_path_for_conversion.exists():
                    # 创建隐藏目录存放所有文件
                    temp_dir = output_path.parent / ".temp"
                    temp_dir.mkdir(exist_ok=True)
                    
                    # 修改.vtt输出路径到隐藏目录
                    output_vtt_filename = f"{output_path.stem}.vtt"
                    output_vtt_path = temp_dir / output_vtt_filename
                    
                    # 1. 创建调整后的VTT文件 - 使用映射后的时间
                    adjusted_vtt_content = ""
                    try:
                        # 将segments转换为便于查找的格式
                        segment_duration_map = []
                        output_time_position = 0.0
                        for seg in segments:
                            if seg['end'] <= seg['start']:
                                continue
                            segment_duration = seg['end'] - seg['start']
                            segment_duration_map.append([
                                seg['start'],
                                seg['end'],
                                output_time_position,
                                segment_duration
                            ])
                            output_time_position += segment_duration

                        # 读取源VTT
                        import webvtt
                        vtt_obj = webvtt.read(str(source_vtt_path_for_conversion))
                        adjusted_vtt_content = "WEBVTT\n\n"
                        processed_count = 0

                        # 调整每个字幕的时间
                        for caption in vtt_obj.captions:
                            caption_start_s = caption.start_in_seconds
                            caption_end_s = caption.end_in_seconds
                            caption_text = caption.text.strip()
                            
                            if not caption_text:
                                continue
                                
                            # 在片段中查找此字幕
                            output_start = None
                            output_end = None
                            
                            for seg_info in segment_duration_map:
                                original_start, original_end, output_start_time, duration = seg_info
                                
                                # 字幕完全在片段内
                                if original_start <= caption_start_s and caption_end_s <= original_end:
                                    time_offset_in_segment = caption_start_s - original_start
                                    output_start = output_start_time + time_offset_in_segment
                                    output_end = output_start + (caption_end_s - caption_start_s)
                                    
                                    if output_end > output_start_time + duration:
                                        output_end = output_start_time + duration
                                    break
                                    
                                # 字幕部分在片段内（开始点在片段内）
                                elif original_start <= caption_start_s < original_end:
                                    time_offset_in_segment = caption_start_s - original_start
                                    output_start = output_start_time + time_offset_in_segment
                                    output_end = output_start_time + duration
                                    break
                                    
                                # 字幕部分在片段内（结束点在片段内）
                                elif original_start < caption_end_s <= original_end:
                                    output_start = output_start_time
                                    output_end = output_start_time + (caption_end_s - original_start)
                                    break
                                    
                                # 字幕完全覆盖了片段
                                elif caption_start_s <= original_start and caption_end_s >= original_end:
                                    output_start = output_start_time
                                    output_end = output_start_time + duration
                                    break
                                    
                            if output_start is None or output_end is None or output_start >= output_end:
                                continue
                                
                            # 格式化为VTT时间格式
                            def format_vtt_time(seconds):
                                hours = int(seconds // 3600)
                                minutes = int((seconds % 3600) // 60)
                                secs = int(seconds % 60)
                                msecs = int((seconds - int(seconds)) * 1000)
                                return f"{hours:02d}:{minutes:02d}:{secs:02d}.{msecs:03d}"
                                
                            start_str = format_vtt_time(output_start)
                            end_str = format_vtt_time(output_end)
                            
                            # 添加字幕到VTT内容
                            adjusted_vtt_content += f"{start_str} --> {end_str}\n{caption_text}\n\n"
                            processed_count += 1
                            
                        # 保存调整后的VTT
                        with open(output_vtt_path, 'w', encoding='utf-8') as vtt_file:
                            vtt_file.write(adjusted_vtt_content)
                        
                        # 添加到临时文件列表中进行清理
                        temp_files_to_clean.append(output_vtt_path)
                            
                        logger.info(f"[Job {job_id}] Created adjusted VTT file with {processed_count} captions: {output_vtt_path}")
                        
                        # 保存VTT文件路径以在响应中返回
                        subtitle_vtt_path = str(output_vtt_path.relative_to(DATA_DIR))
                        output_files["subtitle_vtt"] = subtitle_vtt_path
                        
                    except Exception as e:
                        logger.error(f"[Job {job_id}] Error creating adjusted VTT file: {e}", exc_info=True)
                    
                    # 2. 生成ASS文件用于可选的烧录过程
                    # 使用隐藏目录存储所有临时文件
                    temp_dir = output_path.parent / ".temp"
                    temp_dir.mkdir(exist_ok=True)
                    temp_ass_path = temp_dir / f"subtitle_{job_id}.ass"
                    temp_files_to_clean.append(temp_ass_path)
                    
                    # Convert subtitle file to ASS for embedding
                    if source_subtitle_path_for_conversion and source_subtitle_path_for_conversion.exists():
                        if subtitle_type == 'srt':
                            # Convert SRT to ASS
                            conversion_success = convert_srt_to_ass(
                                str(source_subtitle_path_for_conversion), 
                                str(temp_ass_path),
                                segments=segments
                            )
                        else:
                            # Convert VTT to ASS (existing logic)
                            conversion_success = convert_vtt_to_ass(
                                str(source_subtitle_path_for_conversion), 
                                str(temp_ass_path),
                                segments=segments,
                                is_bilingual=(embed_subtitle_lang == 'bilingual')
                            )
                    else:
                        conversion_success = False
                    
                    if conversion_success:
                        logger.info(f"[Job {job_id}] Successfully converted {subtitle_type.upper()} to ASS: {temp_ass_path}")
                        
                        # 保存ASS文件到.temp目录中，而不是最终输出目录
                        output_ass_filename = f"{output_path.stem}.ass"
                        output_ass_path = temp_dir / output_ass_filename
                        import shutil
                        shutil.copy2(temp_ass_path, output_ass_path)
                        
                        # 添加到临时文件列表中进行清理
                        temp_files_to_clean.append(output_ass_path)
                        
                        subtitle_ass_path = str(output_ass_path.relative_to(DATA_DIR))
                        output_files["subtitle_ass"] = subtitle_ass_path
                        logger.info(f"[Job {job_id}] Saved final ASS file to .temp directory: {output_ass_path}")
                        
                        # 为烧录字幕准备filter
                        escaped_ass_path = str(temp_ass_path).replace(':', '\\:').replace('\\', '\\\\')
                        
                        # 添加系统字体目录
                        font_dirs = [
                            "/System/Library/Fonts",
                            "/Library/Fonts",
                            f"{os.path.expanduser('~')}/Library/Fonts"
                        ]
                        
                        valid_font_dirs = [d for d in font_dirs if os.path.exists(d)]
                        
                        if valid_font_dirs:
                            fontsdir_option = f":fontsdir='{valid_font_dirs[0]}'"
                            subtitle_filter = f"ass='{escaped_ass_path}'{fontsdir_option}" 
                        else:
                            subtitle_filter = f"ass='{escaped_ass_path}'" 
                            
                        logger.info(f"[Job {job_id}] Using ASS filter: {subtitle_filter}")
                    else:
                        logger.error(f"[Job {job_id}] Failed to convert VTT to ASS. No subtitles will be embedded.")
                        subtitle_filter = None
                else:
                    logger.warning(f"[Job {job_id}] No source VTT file available for conversion ('{embed_subtitle_lang}'). Proceeding without subtitles.")
                    subtitle_filter = None

            # --- 准备视频剪切过程 - 更平滑的解决方案 ---
            # 使用简化的concat方式，避免复杂的filter_complex问题
            temp_segment_files = []
            
            # 创建隐藏的临时目录
            temp_dir = output_path.parent / ".temp"
            temp_dir.mkdir(exist_ok=True)
            temp_files_to_clean.append(temp_dir)
            
            segments_file_path = temp_dir / f"segments_{job_id}.txt"
            temp_files_to_clean.append(segments_file_path)
            
            # 创建片段列表文件
            valid_segments = [seg for seg in segments if seg['end'] > seg['start']]
            if not valid_segments:
                raise ValueError("No valid segments found after filtering.")
                
            # 为每个片段创建临时文件
            with open(segments_file_path, 'w') as f:
                for i, seg in enumerate(valid_segments):
                    start_time = seg['start']
                    end_time = seg['end']
                    duration = end_time - start_time
                    
                    # 切出单个片段到临时文件
                    temp_file_suffix = ".wav" if output_format == 'wav' else ".mp4"
                    temp_file = temp_dir / f"segment_{job_id}_{i}{temp_file_suffix}"
                    temp_segment_files.append(temp_file)
                    temp_files_to_clean.append(temp_file)
                    
                    # 根据输出格式选择不同的命令
                    if output_format == 'wav':
                        # WAV 音频提取命令
                        segment_cmd = [
                            "ffmpeg", "-v", "error",
                            "-ss", str(start_time),
                            "-t", str(duration),
                            "-i", str(input_path),
                            "-acodec", "pcm_s16le",  # WAV 编码
                            "-ar", "16000",          # 采样率
                            "-ac", "1",              # 单声道
                            "-y", str(temp_file)
                        ]
                    else:
                        # 视频切分命令（保持原有逻辑）
                        segment_cmd = [
                            "ffmpeg", "-v", "error",
                            "-ss", str(start_time),
                            "-t", str(duration),
                            "-i", str(input_path),
                            "-c:v", "libx264", "-preset", "fast",
                            "-c:a", "aac",
                            "-avoid_negative_ts", "1",
                            "-y", str(temp_file)
                        ]
                    
                    logger.info(f"[Job {job_id}] Extracting segment {i}: {' '.join(shlex.quote(str(c)) for c in segment_cmd)}")
                    
                    # 执行片段切分
                    try:
                        seg_process = subprocess.run(
                            segment_cmd,
                            capture_output=True,
                            text=True,
                            check=False,
                            encoding='utf-8'
                        )
                        
                        if seg_process.returncode != 0:
                            logger.error(f"[Job {job_id}] Failed to extract segment {i}. Error: {seg_process.stderr}")
                            continue
                            
                        # 如果成功创建了片段，添加到列表文件
                        if temp_file.exists() and temp_file.stat().st_size > 1024:
                            f.write(f"file '{temp_file.resolve()}'\n")
                        else:
                            logger.warning(f"[Job {job_id}] Segment {i} file missing or too small, skipping")
                            
                    except Exception as e:
                        logger.error(f"[Job {job_id}] Error extracting segment {i}: {e}", exc_info=True)
                        
            # 检查是否有有效片段
            if not temp_segment_files or not segments_file_path.exists() or segments_file_path.stat().st_size == 0:
                error_message = "Failed to extract any valid segments"
                logger.error(f"[Job {job_id}] {error_message}")
                cut_job_statuses[job_id]["status"] = "failed"
                cut_job_statuses[job_id]["message"] = error_message
                return
            
            # 拼接所有片段
            logger.info(f"[Job {job_id}] Concatenating segments using segments file")
            
            # 根据输出格式选择不同的处理流程
            if output_format == 'wav':
                # 对于 WAV 格式，直接拼接到最终输出文件
                temp_concat_file = output_path
                
                concat_cmd = [
                    "ffmpeg", "-v", "info",
                    "-f", "concat",
                    "-safe", "0",
                    "-i", str(segments_file_path),
                    "-c", "copy",
                    "-y", str(temp_concat_file)
                ]
                
                logger.info(f"[Job {job_id}] Running WAV concat command: {' '.join(shlex.quote(str(c)) for c in concat_cmd)}")
                
                # 执行拼接
                concat_process = subprocess.run(
                    concat_cmd,
                    capture_output=True,
                    text=True,
                    check=False,
                    encoding='utf-8'
                )
                
                if concat_process.returncode != 0:
                    error_message = f"Failed to concatenate WAV segments. Error: {concat_process.stderr}"
                    logger.error(f"[Job {job_id}] {error_message}")
                    cut_job_statuses[job_id]["status"] = "failed"
                    cut_job_statuses[job_id]["message"] = error_message
                    return
                
                # WAV 处理完成，不需要后续字幕处理
                logger.info(f"[Job {job_id}] WAV processing completed successfully")
                
            else:
                # 视频处理流程（保持原有逻辑）
                # 首先创建无字幕版本
                temp_concat_file = temp_dir / f"concat_{job_id}.mp4"
                temp_files_to_clean.append(temp_concat_file)
                
                concat_cmd = [
                    "ffmpeg", "-v", "info",
                    "-f", "concat",
                    "-safe", "0",
                    "-i", str(segments_file_path),
                    "-c", "copy",
                    "-movflags", "+faststart",
                    "-y", str(temp_concat_file)
                ]
                
                logger.info(f"[Job {job_id}] Running video concat command: {' '.join(shlex.quote(str(c)) for c in concat_cmd)}")
                
                # 执行拼接
                concat_process = subprocess.run(
                    concat_cmd,
                    capture_output=True,
                    text=True,
                    check=False,
                    encoding='utf-8'
                )
                
                if concat_process.returncode != 0:
                    error_message = f"Failed to concatenate video segments. Error: {concat_process.stderr}"
                    logger.error(f"[Job {job_id}] {error_message}")
                    cut_job_statuses[job_id]["status"] = "failed"
                    cut_job_statuses[job_id]["message"] = error_message
                    return
                
                # 如果需要添加字幕，处理拼接好的视频
                if subtitle_filter:
                    logger.info(f"[Job {job_id}] Adding subtitles to concatenated video")
                    
                    burn_cmd = [
                        "ffmpeg", "-v", "info",
                        "-i", str(temp_concat_file),
                        "-vf", subtitle_filter,
                        "-c:v", "libx264", "-preset", "fast",
                        "-c:a", "copy",
                        "-movflags", "+faststart",
                        "-y", str(output_path)
                    ]
                    
                    logger.info(f"[Job {job_id}] Running subtitle burn command: {' '.join(shlex.quote(str(c)) for c in burn_cmd)}")
                    
                    # 执行字幕烧录
                    burn_process = subprocess.run(
                        burn_cmd,
                        capture_output=True,
                        text=True,
                        check=False,
                        encoding='utf-8'
                    )
                    
                    if burn_process.returncode == 0:
                        logger.info(f"[Job {job_id}] Successfully added subtitles to video")
                        output_files["has_burned_subtitles"] = True
                    else:
                        logger.error(f"[Job {job_id}] Failed to add subtitles. Error: {burn_process.stderr}")
                        logger.info(f"[Job {job_id}] Using concatenated video without subtitles")
                        
                        # 使用无字幕版本
                        try:
                            import shutil
                            shutil.copy2(temp_concat_file, output_path)
                            output_files["has_burned_subtitles"] = False
                            logger.info(f"[Job {job_id}] Copied non-subtitled video to output")
                        except Exception as e:
                            error_message = f"Failed to copy concatenated video: {e}"
                            logger.error(f"[Job {job_id}] {error_message}")
                            cut_job_statuses[job_id]["status"] = "failed"
                            cut_job_statuses[job_id]["message"] = error_message
                            return
                else:
                    # 直接使用拼接好的无字幕视频
                    try:
                        import shutil
                        shutil.copy2(temp_concat_file, output_path)
                        output_files["has_burned_subtitles"] = False
                        logger.info(f"[Job {job_id}] Copied concatenated video to output (no subtitles)")
                    except Exception as e:
                        error_message = f"Failed to copy concatenated video: {e}"
                        logger.error(f"[Job {job_id}] {error_message}")
                        cut_job_statuses[job_id]["status"] = "failed"
                        cut_job_statuses[job_id]["message"] = error_message
                        return
                    
            # 检查输出文件
            if not output_path.exists():
                error_message = "Output file was not created"
                logger.error(f"[Job {job_id}] {error_message}")
                cut_job_statuses[job_id]["status"] = "failed"
                cut_job_statuses[job_id]["message"] = error_message
                return
                
            file_size = output_path.stat().st_size
            if file_size < 1024:  # 文件小于1KB，可能是空文件
                error_message = f"Output file is too small ({file_size} bytes). Possible error."
                logger.error(f"[Job {job_id}] {error_message}")
                cut_job_statuses[job_id]["status"] = "failed"
                cut_job_statuses[job_id]["message"] = error_message
                try:
                    output_path.unlink()
                except OSError:
                    pass
                return
            
            # 更新任务状态为完成
            cut_job_statuses[job_id]["status"] = "completed"
            cut_job_statuses[job_id]["output_path"] = str(output_path.relative_to(DATA_DIR))
            cut_job_statuses[job_id]["output_files"] = output_files
            
            # 根据输出格式显示不同的完成消息
            if output_format == 'wav':
                cut_job_statuses[job_id]["message"] = "Audio processing completed successfully."
                logger.info(f"[Job {job_id}] Audio processing completed successfully with outputs: {output_files}")
            else:
                cut_job_statuses[job_id]["message"] = "Video processing completed successfully."
                logger.info(f"[Job {job_id}] Video processing completed successfully with outputs: {output_files}")

        except Exception as e:
            error_msg = f"Error during ffmpeg cut task: {e}"
            logger.error(f"[Job {job_id}] {error_msg}", exc_info=True)
            cut_job_statuses[job_id]["status"] = "failed"
            cut_job_statuses[job_id]["message"] = error_msg
            cut_job_statuses[job_id].pop("output_path", None)
        
        finally:
            # 确保在所有情况下都执行清理
            cleanup_temp_files()

# --- API Endpoints for Cutting ---

@app.post("/api/tasks/{task_uuid}/cut", response_model=CutResponse, status_code=202)
async def start_video_cut(
    task_uuid: UUID,
    request: CutRequest, # Updated to use the modified CutRequest model
    background_tasks: BackgroundTasks
):
    """Starts an asynchronous video cutting job."""
    global cut_job_statuses
    task_uuid_str = str(task_uuid)
    logger.info(f"Received cut request for task {task_uuid_str} with {len(request.segments)} segments. Embed: {request.embed_subtitle_lang}, Format: {request.output_format}")

    # 1. Load Metadata
    all_metadata = await load_metadata()
    task_meta = all_metadata.get(task_uuid_str)
    if not task_meta:
        raise HTTPException(status_code=404, detail="Task not found")

    # --- Get VTT files info needed for background task --- 
    task_vtt_files = task_meta.vtt_files if task_meta.vtt_files else {}
    # -----------------------------------------------------
    
    # 2. Validate Media Identifier and Find Input Path
    #    (Assuming media_identifier is the relative path within the task dir)
    input_rel_path_str = request.media_identifier
    if not input_rel_path_str: # Basic check
         raise HTTPException(status_code=400, detail="media_identifier is required.")
    
    input_abs_path = DATA_DIR / input_rel_path_str # Assume media_identifier is relative to DATA_DIR
    if not input_abs_path.is_file(): # Check if it exists and is a file
        logger.error(f"Input media file not found at expected path: {input_abs_path}")
        raise HTTPException(status_code=404, detail=f"Media file not found at path: {input_rel_path_str}")

    # 3. Generate Job ID and Output Path
    job_id = str(uuid.uuid4())
    input_path_obj = Path(input_rel_path_str)
    output_filename = f"{input_path_obj.stem}_cut_{job_id}{input_path_obj.suffix}"
    output_abs_path = DATA_DIR / task_uuid_str / output_filename

    # 4. Initialize Job Status
    cut_job_statuses[job_id] = {
        "status": "pending",
        "message": "Job queued for processing.",
        "task_uuid": task_uuid_str,
        "output_path": None, # Will be filled upon completion
        "embed_lang_requested": request.embed_subtitle_lang # Store requested lang for info
    }

    # 5. Add FFMPEG task to background
    background_tasks.add_task(
        run_ffmpeg_cut,
        task_uuid_str=task_uuid_str,
        input_path=input_abs_path,
        output_path=output_abs_path,
        segments=[seg.dict() for seg in request.segments], # Pass segment data
        job_id=job_id,
        embed_subtitle_lang=request.embed_subtitle_lang, # Pass the lang preference
        vtt_files=task_vtt_files, # Pass VTT file info
        output_format=request.output_format,
        subtitle_type=request.subtitle_type # Add subtitle type parameter
    )

    logger.info(f"Queued cut job {job_id} for task {task_uuid_str}. Output target: {output_abs_path}")

    # 6. Return Accepted response
    return CutResponse(
        job_id=job_id,
        status="processing", # Inform client it's started (or about to start)
        message="Video cutting job started in background."
    )

@app.get("/api/tasks/{task_uuid}/cut/{job_id}/status", response_model=CutStatusResponse)
async def get_cut_job_status(task_uuid: UUID, job_id: str):
    """Gets the status of a specific cutting job."""
    global cut_job_statuses
    task_uuid_str = str(task_uuid)
    logger.debug(f"Checking status for cut job {job_id} (Task: {task_uuid_str})")

    job_info = cut_job_statuses.get(job_id)

    if not job_info:
        logger.warning(f"Cut job {job_id} not found.")
        raise HTTPException(status_code=404, detail="Cut job not found")

    # Optional: Check if the job belongs to the requested task_uuid
    if job_info.get("task_uuid") != task_uuid_str:
        logger.warning(f"Cut job {job_id} belongs to task {job_info.get('task_uuid')}, not requested task {task_uuid_str}.")
        raise HTTPException(status_code=404, detail="Cut job not found for this task") # Treat as not found for security

    return CutStatusResponse(
        job_id=job_id,
        status=job_info.get("status", "unknown"),
        message=job_info.get("message"),
        output_path=job_info.get("output_path") # Will be None unless status is "completed"
    )

# <<< Add New Endpoints Here >>> (Place the new endpoints above this marker if it exists)

from .routes import chat, tasks
app.include_router(chat.router)
app.include_router(tasks.router)

# --- START: Add POST endpoint for file creation/update ---
@app.post("/api/tasks/{task_uuid}/files/{filename}", status_code=200)
async def create_or_update_file(
    task_uuid: UUID, 
    filename: str, 
    file_content: str = Body(..., media_type="text/plain")
):
    """
    Create or update a file in the task's data directory.
    Used primarily for markdown files editing.
    """
    task_uuid_str = str(task_uuid)
    logger.info(f"Request to create/update file '{filename}' for task {task_uuid_str}")

    # Basic security check: prevent path traversal
    if ".." in filename or filename.startswith("/"):
        logger.warning(f"Detected illegal filename request: {filename} (task: {task_uuid_str})")
        raise HTTPException(status_code=400, detail="Illegal filename")

    # Build the full path
    task_data_dir = DATA_DIR / task_uuid_str
    file_path = task_data_dir / filename

    # Ensure task directory exists
    if not task_data_dir.exists() or not task_data_dir.is_dir():
        logger.warning(f"Task data directory not found: {task_data_dir}")
        raise HTTPException(status_code=404, detail="Task data directory not found")

    # Ensure the file path stays within task directory
    try:
        expected_task_dir = (DATA_DIR / task_uuid_str).resolve(strict=True)
        # Create parent directories if needed
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Resolve after potential parent directory creation
        resolved_file_path = await run_in_threadpool(file_path.resolve)
        
        if not str(resolved_file_path).startswith(str(expected_task_dir)):
            logger.warning(f"Path Traversal Check Failed: Resolved path {resolved_file_path} does not start with expected directory {expected_task_dir}")
            raise HTTPException(status_code=400, detail="File path outside allowed directory")
    except Exception as e:
        logger.error(f"Error checking or creating file path: {file_path} - {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

    # Write the content to the file
    try:
        async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
            await f.write(file_content)
        logger.info(f"Successfully created/updated file: {file_path}")
        return {"message": f"File {filename} created/updated successfully"}
    except Exception as e:
        logger.error(f"Error writing file {file_path}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to write file: {str(e)}")
# --- END: Add POST endpoint for file creation/update ---

# --- SRT Parsing and Conversion Functions ---

def parse_srt_file(srt_file_path: Path) -> List[Dict]:
    """Parse SRT file and return list of subtitle entries"""
    try:
        with open(srt_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Split by double newlines to get subtitle blocks
        blocks = content.strip().split('\n\n')
        subtitles = []
        
        for block in blocks:
            lines = block.strip().split('\n')
            if len(lines) < 3:
                continue
                
            # Parse subtitle number (first line)
            try:
                subtitle_num = int(lines[0])
            except ValueError:
                continue
                
            # Parse timestamp (second line)
            if '-->' not in lines[1]:
                continue
                
            time_parts = lines[1].split(' --> ')
            if len(time_parts) != 2:
                continue
                
            start_time = srt_time_to_seconds(time_parts[0].strip())
            end_time = srt_time_to_seconds(time_parts[1].strip())
            
            # Parse text (remaining lines)
            text = '\n'.join(lines[2:]).strip()
            
            subtitles.append({
                'start': start_time,
                'end': end_time,
                'text': text
            })
            
        logger.info(f"Parsed {len(subtitles)} subtitles from SRT file: {srt_file_path}")
        return subtitles
        
    except Exception as e:
        logger.error(f"Error parsing SRT file {srt_file_path}: {e}")
        return []

def srt_time_to_seconds(time_str: str) -> float:
    """Convert SRT time format (HH:MM:SS,mmm) to seconds"""
    try:
        # Handle both comma and dot as decimal separator
        time_str = time_str.replace(',', '.')
        parts = time_str.split(':')
        if len(parts) != 3:
            return 0.0
            
        hours = int(parts[0])
        minutes = int(parts[1])
        seconds_parts = parts[2].split('.')
        seconds = int(seconds_parts[0])
        milliseconds = int(seconds_parts[1]) if len(seconds_parts) > 1 else 0
        
        return hours * 3600 + minutes * 60 + seconds + milliseconds / 1000.0
    except Exception:
        return 0.0

def seconds_to_srt_time(seconds: float) -> str:
    """Convert seconds to SRT time format (HH:MM:SS,mmm)"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    milliseconds = int((seconds - int(seconds)) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"

def create_adjusted_srt_file(source_srt_path: Path, output_srt_path: Path, segments: List[Dict]) -> bool:
    """Create adjusted SRT file with remapped timestamps based on segments"""
    try:
        # Parse source SRT
        subtitles = parse_srt_file(source_srt_path)
        if not subtitles:
            logger.error(f"No subtitles found in source SRT: {source_srt_path}")
            return False
            
        # Create segment mapping
        segment_duration_map = []
        output_time_position = 0.0
        for seg in segments:
            if seg['end'] <= seg['start']:
                continue
            segment_duration = seg['end'] - seg['start']
            segment_duration_map.append([
                seg['start'],
                seg['end'],
                output_time_position,
                segment_duration
            ])
            output_time_position += segment_duration
            
        # Process subtitles
        adjusted_subtitles = []
        subtitle_counter = 1
        
        for subtitle in subtitles:
            subtitle_start = subtitle['start']
            subtitle_end = subtitle['end']
            subtitle_text = subtitle['text'].strip()
            
            if not subtitle_text:
                continue
                
            # Find which segment this subtitle belongs to
            output_start = None
            output_end = None
            
            for seg_info in segment_duration_map:
                original_start, original_end, output_start_time, duration = seg_info
                
                # Subtitle completely within segment
                if original_start <= subtitle_start and subtitle_end <= original_end:
                    time_offset_in_segment = subtitle_start - original_start
                    output_start = output_start_time + time_offset_in_segment
                    output_end = output_start + (subtitle_end - subtitle_start)
                    
                    if output_end > output_start_time + duration:
                        output_end = output_start_time + duration
                    break
                    
                # Subtitle partially in segment (start point in segment)
                elif original_start <= subtitle_start < original_end:
                    time_offset_in_segment = subtitle_start - original_start
                    output_start = output_start_time + time_offset_in_segment
                    output_end = output_start_time + duration
                    break
                    
                # Subtitle partially in segment (end point in segment)
                elif original_start < subtitle_end <= original_end:
                    output_start = output_start_time
                    output_end = output_start_time + (subtitle_end - original_start)
                    break
                    
                # Subtitle completely covers segment
                elif subtitle_start <= original_start and subtitle_end >= original_end:
                    output_start = output_start_time
                    output_end = output_start_time + duration
                    break
                    
            if output_start is None or output_end is None or output_start >= output_end:
                continue
                
            adjusted_subtitles.append({
                'number': subtitle_counter,
                'start': output_start,
                'end': output_end,
                'text': subtitle_text
            })
            subtitle_counter += 1
            
        # Write adjusted SRT file
        with open(output_srt_path, 'w', encoding='utf-8') as f:
            for subtitle in adjusted_subtitles:
                f.write(f"{subtitle['number']}\n")
                f.write(f"{seconds_to_srt_time(subtitle['start'])} --> {seconds_to_srt_time(subtitle['end'])}\n")
                f.write(f"{subtitle['text']}\n\n")
                
        logger.info(f"Created adjusted SRT file with {len(adjusted_subtitles)} subtitles: {output_srt_path}")
        return True
        
    except Exception as e:
        logger.error(f"Error creating adjusted SRT file: {e}", exc_info=True)
        return False

# --- End SRT Functions ---

# 将SRT转换为ASS格式的函数
def convert_srt_to_ass(srt_file_path, output_ass_path, segments: List[Dict]):
    try:
        from pathlib import Path
        import re
        
        # 确保输出目录存在
        Path(output_ass_path).parent.mkdir(parents=True, exist_ok=True)
        
        # 解析SRT文件
        subtitles = parse_srt_file(Path(srt_file_path))
        if not subtitles:
            logger.error(f"No subtitles found in SRT file: {srt_file_path}")
            return False
            
        logger.info(f"转换SRT到ASS: 读取到{len(subtitles)}个字幕条目...")
        
        # 将segments时间范围重组为更方便处理的格式
        segment_duration_map = []  # [原片段起始, 原片段结束, 输出片段起始, 输出片段延续时长]
        output_time_position = 0.0
        
        for seg in segments:
            if seg['end'] <= seg['start']:
                continue  # 跳过无效片段
            
            segment_duration = seg['end'] - seg['start']
            segment_duration_map.append([
                seg['start'],         # 原片段起始时间
                seg['end'],           # 原片段结束时间
                output_time_position, # 该片段在输出视频中的起始位置
                segment_duration      # 该片段持续时间
            ])
            output_time_position += segment_duration
        
        # 将时间戳从秒转换为ASS格式 (h:mm:ss.cc)
        def format_time(seconds_val):
            h = int(seconds_val / 3600)
            m = int((seconds_val % 3600) / 60)
            s = int(seconds_val % 60)
            cs = int((seconds_val - int(seconds_val)) * 100)  # ASS使用厘秒(centiseconds)
            return f"{h}:{m:02d}:{s:02d}.{cs:02d}"
        
        # 开始构建ASS内容
        ass_content = """[Script Info]
Title: Generated from SRT
ScriptType: v4.00+

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,20,&H00FFFFFF,&H000000FF,&H00000000,&H80000000,0,0,0,0,100,100,0,0,1,2,0,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""
        
        included_subtitle_count = 0
        
        for subtitle in subtitles:
            original_start = subtitle['start']
            original_end = subtitle['end']
            text = subtitle['text'].strip()
            
            if not text:
                continue
            
            # 在片段中查找此字幕
            output_start = None
            output_end = None
            
            for seg_info in segment_duration_map:
                seg_start, seg_end, output_start_time, duration = seg_info
                
                # 字幕完全在片段内
                if seg_start <= original_start and original_end <= seg_end:
                    time_offset_in_segment = original_start - seg_start
                    output_start = output_start_time + time_offset_in_segment
                    output_end = output_start + (original_end - original_start)
                    
                    if output_end > output_start_time + duration:
                        output_end = output_start_time + duration
                    break
                    
                # 字幕部分在片段内（开始点在片段内）
                elif seg_start <= original_start < seg_end:
                    time_offset_in_segment = original_start - seg_start
                    output_start = output_start_time + time_offset_in_segment
                    output_end = output_start_time + duration
                    break
                    
                # 字幕部分在片段内（结束点在片段内）
                elif seg_start < original_end <= seg_end:
                    output_start = output_start_time
                    output_end = output_start_time + (original_end - seg_start)
                    break
                    
                # 字幕完全覆盖了片段
                elif original_start <= seg_start and original_end >= seg_end:
                    output_start = output_start_time
                    output_end = output_start_time + duration
                    break
            
            if output_start is None or output_end is None or output_start >= output_end:
                continue
            
            # 格式化时间
            start_time_str = format_time(output_start)
            end_time_str = format_time(output_end)
            
            # 清理文本，转义特殊字符
            clean_text = text.replace('\n', '\\N').replace('{', '\\{').replace('}', '\\}')
            
            # 添加到ASS内容
            ass_content += f"Dialogue: 0,{start_time_str},{end_time_str},Default,,0,0,0,,{clean_text}\n"
            included_subtitle_count += 1
        
        # 写入ASS文件
        with open(output_ass_path, 'w', encoding='utf-8') as f:
            f.write(ass_content)
        
        logger.info(f"成功转换SRT到ASS: {output_ass_path} (包含 {included_subtitle_count} 条字幕)")
        return True
        
    except Exception as e:
        logger.error(f"转换SRT到ASS时发生错误: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    import uvicorn
    # from pydantic import ValidationError # Already imported earlier if needed
    uvicorn.run(app, host="0.0.0.0", port=8000) 